{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4470d9",
   "metadata": {},
   "source": [
    "# Combine IDOC population data sets\n",
    "\n",
    "IDOC publishes population data sets which consist of each person housed in a state prison each quarter and information about them, such as their demographics, the type of crime they're being held for, the date on which they were apprehended, etc. The data sets are .xls files, but the titles, URLs, and data within each data set differ and need to be cleaned. This script combines all population data sets and makes the information within them consistent.\n",
    "\n",
    "The population data sets can be found here: https://www2.illinois.gov/idoc/reportsandstatistics/Pages/Prison-Population-Data-Sets.aspx\n",
    "\n",
    "This was last updated with the Dec 2021 data set as the most recently published data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d878d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import warnings\n",
    "from datetime import datetime as dt, date, timedelta as td\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlrd\n",
    "\n",
    "%matplotlib inline\n",
    "inline_rc = dict(mpl.rcParams)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990f6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# helper function to get list of hex codes from tableau 20 pallette\n",
    "def get_hex(n):\n",
    "    cmap = cm.get_cmap('tab20', n)\n",
    "    return [mcolors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n",
    "\n",
    "# helper function to get rgb code from hex code\n",
    "def get_rgb_from_hex(hex_code):\n",
    "    hex_code = hex_code.lstrip('#')\n",
    "    return tuple(int(hex_code[i:i+2], 16) for i in (0, 2, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad5919",
   "metadata": {},
   "source": [
    "Get links to the population data sets from the IDOC site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf94ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated February 6, 2026 to account for the new format of the links on teh IDOC website.\n",
    "# The links are now in the format of \"https://idoc.illinois.gov/content/dam/soi/en/web/idoc/reportsandstatistics/prison-population-data-sets/2025/2025-01.xlsx\" instead of \"https://idoc.illinois.gov/reportsandstatistics/prison-population-data-sets/2025-01.xlsx\". The code now uses the urljoin function to join the base URL with the relative URL of the Excel files.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "PAGE_URL = \"https://idoc.illinois.gov/reportsandstatistics/prison-population-data-sets.html\"\n",
    "\n",
    "response = requests.get(PAGE_URL, timeout=60)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "files = []\n",
    "\n",
    "date_pattern = re.compile(r'on (\\d{2}-\\d{2}-\\d{2})')\n",
    "\n",
    "for a in soup.select(\"a[href]\"):\n",
    "    label = a.get_text(\" \", strip=True)\n",
    "    href = a[\"href\"]\n",
    "\n",
    "    if re.search(r\"\\.(xls|xlsx)$\", href, re.I):\n",
    "\n",
    "        match = date_pattern.search(label)\n",
    "\n",
    "        if match:\n",
    "            doc_date = pd.to_datetime(match.group(1), format=\"%m-%d-%y\")\n",
    "\n",
    "            files.append({\n",
    "                \"date\": doc_date,\n",
    "                \"url\": urljoin(PAGE_URL, href),\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69d5ad",
   "metadata": {},
   "source": [
    "Read in and combine the data sets. This takes a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ea671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def read_idoc_excel(url):\n",
    "    \n",
    "    r = requests.get(url, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    raw = pd.read_excel(BytesIO(r.content), header=None, engine=\"xlrd\")\n",
    "\n",
    "    # find header row dynamically\n",
    "    header_row = None\n",
    "    for i in range(40):\n",
    "        if (raw.iloc[i].astype(str).str.contains(\"IDOC\")).any():\n",
    "            header_row = i\n",
    "            break\n",
    "\n",
    "    if header_row is None:\n",
    "        raise ValueError(f\"Could not find header in {url}\")\n",
    "\n",
    "    df = pd.read_excel(\n",
    "        BytesIO(r.content),\n",
    "        skiprows=header_row,\n",
    "        dtype=\"object\",\n",
    "        engine=\"xlrd\"\n",
    "    )\n",
    "\n",
    "    # drop garbage unnamed cols\n",
    "    df = df.loc[:, ~df.columns.astype(str).str.contains(\"^Unnamed\")]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "\n",
    "    print(f\"{i}/{len(files)} — {file['date'].date()}\")\n",
    "\n",
    "    df = read_idoc_excel(file[\"url\"])\n",
    "\n",
    "    df[\"Doc Date\"] = file[\"date\"]\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96f950",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0fc605f",
   "metadata": {},
   "source": [
    "Export file so I don't have to read these in every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('raw_full_pop_data.csv')\n",
    "\n",
    "# df = pd.read_csv('raw_full_pop_data.csv', dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0ada5",
   "metadata": {},
   "source": [
    "Remove notes at the end of each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5860d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize column names immediately after reading ---\n",
    "df.columns = (\n",
    "    df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    ")\n",
    "\n",
    "# --- Ensure expected column exists ---\n",
    "if \"name\" not in df.columns:\n",
    "    raise ValueError(\"Expected column 'name' not found — schema may have changed.\")\n",
    "\n",
    "# --- Drop blank rows safely ---\n",
    "df = df.loc[df[\"name\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# --- Optional: create a stable row id (ONLY if you truly need it) ---\n",
    "df.insert(0, \"row_id\", range(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf965f3",
   "metadata": {},
   "source": [
    "**Explore the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849a13c",
   "metadata": {},
   "source": [
    "**Dedupe/combine columns**\n",
    "\n",
    "Sometimes, columns in the individual data sets had different names. Combine these into a single column in the combined version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde03ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_cols(cols):\n",
    "    \"\"\"Normalize column names to reduce accidental mismatches.\"\"\"\n",
    "    return (\n",
    "        pd.Index(cols)\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "def consolidate_duplicate_columns(\n",
    "    df: pd.DataFrame,\n",
    "    base_names: list[str],\n",
    "    max_suffix: int = 5,\n",
    "    rename_map: dict[str, str] | None = None,\n",
    "    drop_all_null_cols: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each base name, fill nulls in the base column from any suffixed variants:\n",
    "      Base, Base1, Base2, ... Base{max_suffix-1}\n",
    "    Then drop the suffixed columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) normalize existing column names (whitespace etc.)\n",
    "    df.columns = normalize_cols(df.columns)\n",
    "\n",
    "    # 2) optional renames to align known variants\n",
    "    if rename_map:\n",
    "        rename_map = {k.strip(): v.strip() for k, v in rename_map.items()}\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 3) consolidate\n",
    "    for base in base_names:\n",
    "        base = base.strip()\n",
    "        if base not in df.columns:\n",
    "            if verbose:\n",
    "                print(f\"\\n[skip] Base column not found: {base!r}\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nCombining dupes for: {base}\")\n",
    "            print(\"  start non-null:\", df[base].notna().sum())\n",
    "\n",
    "        # find all suffixed columns matching base + digits (e.g., \"Admission Type1\")\n",
    "        # we include \"base\" itself separately; here we only collect suffix variants\n",
    "        pattern = re.compile(rf\"^{re.escape(base)}(\\d+)$\")\n",
    "        suffix_cols = [c for c in df.columns if pattern.match(c)]\n",
    "\n",
    "        # optionally also look for accidental double-space variants already normalized;\n",
    "        # normalization should handle most of these.\n",
    "\n",
    "        # Fill base from each suffix col in order of suffix number\n",
    "        def suffix_num(c):\n",
    "            return int(pattern.match(c).group(1))\n",
    "\n",
    "        for c in sorted(suffix_cols, key=suffix_num):\n",
    "            before = df[base].notna().sum()\n",
    "            df[base] = df[base].fillna(df[c])\n",
    "            after = df[base].notna().sum()\n",
    "            if verbose:\n",
    "                print(f\"  filled from {c}: {before} -> {after}\")\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    # 4) drop fully empty columns\n",
    "    if drop_all_null_cols:\n",
    "        empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "        if verbose:\n",
    "            print(\"\\nDropping empty columns:\")\n",
    "            for c in empty_cols:\n",
    "                print(\" \", c)\n",
    "        if empty_cols:\n",
    "            df = df.drop(columns=empty_cols)\n",
    "\n",
    "    return df\n",
    "def to_snake(s: str) -> str:\n",
    "    return (\n",
    "        str(s)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\" \", \"_\")\n",
    "    )\n",
    "\n",
    "base_names = [\n",
    "    \"Projected Mandatory Supervised Release (MSR) Date\",\n",
    "    \"Projected Discharge Date\",\n",
    "    \"Current Admission Date\",\n",
    "    \"Admission Type\",\n",
    "]\n",
    "base_names = [to_snake(x) for x in base_names]\n",
    "\n",
    "rename_map = {\n",
    "    \"Projected Discharge  Date2\": \"Projected Discharge Date1\",\n",
    "    \"Current Admission Type\": \"Admission Type1\",\n",
    "}\n",
    "# normalize rename_map keys/values too\n",
    "rename_map = {to_snake(k): to_snake(v) for k, v in rename_map.items()}\n",
    "\n",
    "df = consolidate_duplicate_columns(\n",
    "    df,\n",
    "    base_names=base_names,\n",
    "    rename_map=rename_map,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f866a5f",
   "metadata": {},
   "source": [
    "**Explore further**\n",
    "\n",
    "Which fields need cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3eb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop accidental index column if it exists\n",
    "if \"index\" in df.columns:\n",
    "    df = df.drop(columns=[\"index\"])\n",
    "\n",
    "# if you don't need both:\n",
    "# keep row_id, drop idx (or vice versa)\n",
    "if \"idx\" in df.columns:\n",
    "    df = df.drop(columns=[\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee15971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print('\\n')\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbae35",
   "metadata": {},
   "source": [
    "* Sex has nulls and incorrect values\n",
    "* Race has nulls and values that can be combined\n",
    "* Veteran Status has values that can be combined\n",
    "* Parent Institution has values that can be combined\n",
    "* Crime Class has nulls and values that can be combined\n",
    "* Sentencing County has values that can be combined and should be categorized\n",
    "\n",
    "\n",
    "* Truth in Sentencing needs % split into its own column\n",
    "\n",
    "\n",
    "* Sentence Years needs further exploration - includes 0002, 999, LFE\n",
    "* Sentence Months needs further exploration - do these match Sentence Years?\n",
    "\n",
    "\n",
    "* Date of Birth has multiple date formats\n",
    "* Current Admission Date has multiple date formats\n",
    "* Custody Date has multiple date formats\n",
    "* Sentence Date has multiple date formats\n",
    "* Projected Mandatory Supervised Release (MSR) Date has multiple date formats and non-date values\n",
    "* Projected Discharge Date has multiple date formats and non-date values\n",
    "\n",
    "\n",
    "* Admission Type needs further exploration\n",
    "* Holding Offense needs further exploration and should be categorized\n",
    "    * Holding Offense Category exists, but use Phil's categories as well\n",
    "    * Offense Type exists, but use Phil's categories as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce0616",
   "metadata": {},
   "source": [
    "**Clean Sex**\n",
    "\n",
    "Sex contains empty spaces and \"B\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the column first\n",
    "df[\"sex\"] = (\n",
    "    df[\"sex\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\n",
    "            \"B\": \"Unknown\",   # bad code sometimes appears\n",
    "            \"\": \"Unknown\",\n",
    "            \" \": \"Unknown\",\n",
    "            \"nan\": \"Unknown\"\n",
    "        })\n",
    "        .fillna(\"Unknown\")\n",
    ")\n",
    "\n",
    "print(df[\"sex\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c40bf",
   "metadata": {},
   "source": [
    "**Clean Race**\n",
    "\n",
    "Race contains empty spaces, Unknown, and Not Assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d91229",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_map = {\n",
    "    \"W\": \"White\",\n",
    "    \"B\": \"Black\",\n",
    "    \"H\": \"Hispanic\",\n",
    "    \"A\": \"Asian\",\n",
    "    \"I\": \"Indigenous\",\n",
    "    \"O\": \"Other\",\n",
    "}\n",
    "\n",
    "df[\"race\"] = (\n",
    "    df[\"race\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .replace({\n",
    "          \"\": \"Unknown\",\n",
    "          \"Not Assigned\": \"Unknown\",\n",
    "          \"Bi-Racial\": \"Other\",\n",
    "          \"nan\": \"Unknown\",\n",
    "      })\n",
    ")\n",
    "\n",
    "# Final safety catch\n",
    "print(sorted(df[\"race\"].unique()))\n",
    "print(df[\"race\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e357fa",
   "metadata": {},
   "source": [
    "**Clean Veteran Status**\n",
    "\n",
    "Veteran Status contains Yes, No, NaN, and Unknown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if NaN means No or Unknown\n",
    "df[\"veteran_status\"] = (\n",
    "    df[\"veteran_status\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\n",
    "            \"\": pd.NA,\n",
    "            \"nan\": pd.NA\n",
    "        })\n",
    ")\n",
    "\n",
    "# plenty of Nos in the same documents as the NaNs, so NaN likely means \"Unknown\"\n",
    "df[\"veteran_status\"] = df[\"veteran_status\"].fillna(\"Unknown\")\n",
    "\n",
    "status_map = {\n",
    "    \"Yes\": \"Yes\",\n",
    "    \"No\": \"No\",\n",
    "    \"Unknown\": \"Unknown\"\n",
    "}\n",
    "\n",
    "df[\"veteran_status\"] = (\n",
    "    df[\"veteran_status\"]\n",
    "        .map(status_map)\n",
    "        .fillna(\"Unknown\")\n",
    ")\n",
    "\n",
    "df[\"veteran_status\"] = df[\"veteran_status\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc444d",
   "metadata": {},
   "source": [
    "**Parent Intitutions**\n",
    "\n",
    "Clean up the names to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove CC appendix from  names\n",
    "df[\"parent_institution\"] = (\n",
    "    df[\"parent_institution\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s*CC$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "df[\"parent_institution\"] = df[\"parent_institution\"].replace({\n",
    "    \"Southwestern IL\": \"Southwestern Illinois\"\n",
    "})\n",
    "\n",
    "\n",
    "df[\"parent_institution\"] = df[\"parent_institution\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#double checking -- to see if cleaning and standardization worked as expected and to check for any remaining anomalies in the parent_institution column.\n",
    "sorted(df[\"parent_institution\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize dates \n",
    "\n",
    "date_cols = [\n",
    "    \"date_of_birth\",\n",
    "    \"custody_date\",\n",
    "    \"sentence_date\",\n",
    "    \"current_admission_date\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date\",\n",
    "    \"projected_discharge_date\",\n",
    "    \"doc_date\",\n",
    "]\n",
    "\n",
    "for c in date_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert low cardinality columns to category dtype\n",
    "cat_cols = [\"sex\", \"race\", \"veteran_status\", \"admission_type\", \"sentencing_county\", \"parent_institution\"]\n",
    "for c in cat_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09259806",
   "metadata": {},
   "source": [
    "**Clean Crime Class**\n",
    "\n",
    "* Class M is murder\n",
    "* Change empty spaces, NaN, and Missing to Unknown\n",
    "* Unclassified remains as Unclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8994137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"crime_class\"] = (\n",
    "    df[\"crime_class\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\n",
    "            \"Class M\": \"Murder\",\n",
    "            \"\": \"Unknown\",\n",
    "            \"Missing\": \"Unknown\",\n",
    "            \"nan\": \"Unknown\"\n",
    "        })\n",
    "        .fillna(\"Unknown\")\n",
    "        .astype(\"category\")\n",
    ")\n",
    "\n",
    "print(df[\"crime_class\"].value_counts(dropna=False))\n",
    "\n",
    "#checking the unique values in crime_class to see if there are any remaining anomalies or if further standardization is needed.\n",
    "#sorted(df[\"crime_class\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce29fca",
   "metadata": {},
   "source": [
    "**Sentencing Counties**\n",
    "\n",
    "* Clean up different versions of county names\n",
    "* Categorize into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd234ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1) Standardize county strings ---\n",
    "s = (\n",
    "    df[\"sentencing_county\"]\n",
    "      .astype(\"string\")          # keeps missing as <NA>\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "# Fix common variants (case-insensitive)\n",
    "county_fixes = {\n",
    "    \"dekalb\": \"DeKalb\",\n",
    "    \"dupage\": \"DuPage\",\n",
    "    \"lasalle\": \"LaSalle\",\n",
    "    \"out of state\": \"Out of state/Unknown\",\n",
    "    \"unknown\": \"Out of state/Unknown\",\n",
    "}\n",
    "\n",
    "# Apply fixes; if not in map, keep original\n",
    "df[\"sentencing_county\"] = s.str.lower().map(county_fixes).fillna(s)\n",
    "\n",
    "# Optional: collapse blanks to unknown\n",
    "df[\"sentencing_county\"] = df[\"sentencing_county\"].replace([\"\", \" \"], \"Out of state/Unknown\")\n",
    "\n",
    "cook = {\"COOK\"}\n",
    "chi_metro = {\"DUPAGE\", \"LAKE\", \"KANE\", \"MCHENRY\", \"WILL\"}\n",
    "north_il = {\"BOONE\",\"BUREAU\",\"CARROLL\",\"DEKALB\",\"GRUNDY\",\"HENRY\",\"JO DAVIESS\",\"KENDALL\",\"LASALLE\",\"LEE\",\"OGLE\",\n",
    "            \"ROCK ISLAND\",\"STEPHENSON\",\"WHITESIDE\",\"WINNEBAGO\"}\n",
    "central_il = {\"ADAMS\",\"BROWN\",\"CALHOUN\",\"CASS\",\"CHAMPAIGN\",\"CHRISTIAN\",\"COLES\",\"CUMBERLAND\",\"DEWITT\",\"DOUGLAS\",\n",
    "              \"EDGAR\",\"FORD\",\"FULTON\",\"GREENE\",\"HANCOCK\",\"HENDERSON\",\"IROQUOIS\",\"JERSEY\",\"KANKAKEE\",\"KNOX\",\n",
    "              \"LIVINGSTON\",\"LOGAN\",\"MACON\",\"MACOUPIN\",\"MARSHALL\",\"MASON\",\"MCDONOUGH\",\"MCLEAN\",\"MENARD\",\"MERCER\",\n",
    "              \"MONTGOMERY\",\"MORGAN\",\"MOULTRIE\",\"PEORIA\",\"PIATT\",\"PIKE\",\"PUTNAM\",\"SANGAMON\",\"SCHUYLER\",\"SCOTT\",\n",
    "              \"SHELBY\",\"STARK\",\"TAZEWELL\",\"VERMILION\",\"WARREN\",\"WOODFORD\"}\n",
    "st_louis = {\"MADISON\", \"ST. CLAIR\"}\n",
    "south_il = {\"ALEXANDER\",\"BOND\",\"CLARK\",\"CLAY\",\"CLINTON\",\"CRAWFORD\",\"EDWARDS\",\"EFFINGHAM\",\"FAYETTE\",\"FRANKLIN\",\n",
    "            \"GALLATIN\",\"HAMILTON\",\"HARDIN\",\"JACKSON\",\"JASPER\",\"JEFFERSON\",\"JOHNSON\",\"LAWRENCE\",\"MARION\",\"MASSAC\",\n",
    "            \"MONROE\",\"PERRY\",\"POPE\",\"PULASKI\",\"RANDOLPH\",\"RICHLAND\",\"SALINE\",\"UNION\",\"WABASH\",\"WASHINGTON\",\"WAYNE\",\n",
    "            \"WHITE\",\"WILLIAMSON\"}\n",
    "\n",
    "county_upper = df[\"sentencing_county\"].astype(\"string\").str.upper()\n",
    "\n",
    "df[\"sentencing_region\"] = pd.Categorical(\n",
    "    np.select(\n",
    "        [\n",
    "            county_upper.isin(cook),\n",
    "            county_upper.isin(chi_metro),\n",
    "            county_upper.isin(north_il),\n",
    "            county_upper.isin(central_il),\n",
    "            county_upper.isin(st_louis),\n",
    "            county_upper.isin(south_il),\n",
    "        ],\n",
    "        [\n",
    "            \"Cook\",\n",
    "            \"Chicago Metro\",\n",
    "            \"North IL\",\n",
    "            \"Central IL\",\n",
    "            \"St. Louis Metro\",\n",
    "            \"South IL\",\n",
    "        ],\n",
    "        default=\"Out of State/Unknown\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical columns you expect in the final combined dataset\n",
    "SCHEMA = {\n",
    "    \"idoc_#\": \"string\",\n",
    "    \"name\": \"string\",\n",
    "    \"date_of_birth\": \"datetime64[ns]\",\n",
    "    \"sex\": \"category\",\n",
    "    \"race\": \"category\",\n",
    "    \"veteran_status\": \"category\",\n",
    "    \"current_admission_date\": \"datetime64[ns]\",\n",
    "    \"admission_type\": \"category\",\n",
    "    \"parent_institution\": \"category\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date\": \"datetime64[ns]\",\n",
    "    \"projected_discharge_date\": \"datetime64[ns]\",\n",
    "    \"custody_date\": \"datetime64[ns]\",\n",
    "    \"sentence_date\": \"datetime64[ns]\",\n",
    "    \"crime_class\": \"category\",\n",
    "    \"holding_offense\": \"string\",\n",
    "    \"holding_offense_category\": \"category\",\n",
    "    \"offense_type\": \"category\",\n",
    "    \"sentence_years\": \"Int64\",\n",
    "    \"sentence_months\": \"Int64\",\n",
    "    \"truth_in_sentencing\": \"category\",\n",
    "    \"sentencing_county\": \"category\",\n",
    "    \"sentencing_region\": \"category\",\n",
    "    \"doc_date\": \"datetime64[ns]\",\n",
    "}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def enforce_schema(df: pd.DataFrame, schema: dict, *, strict: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) missing columns\n",
    "    missing = [c for c in schema if c not in df.columns]\n",
    "    if missing and strict:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    for c in missing:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "    # 2) unexpected columns (useful warning)\n",
    "    extra = [c for c in df.columns if c not in schema]\n",
    "    if extra and strict:\n",
    "        # in strict mode, fail loudly (best for pipelines)\n",
    "        raise ValueError(f\"Unexpected columns found (schema drift): {extra}\")\n",
    "\n",
    "    # 3) cast types\n",
    "    for col, dtype in schema.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if dtype.startswith(\"datetime\"):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        elif dtype == \"Int64\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        elif dtype == \"string\":\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "        elif dtype == \"category\":\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "        else:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "\n",
    "    # 4) keep columns in canonical order\n",
    "    df = df[list(schema.keys())]\n",
    "    return df\n",
    "df[\"doc_date\"] = pd.to_datetime(df[\"doc_date\"], errors=\"coerce\")\n",
    "\n",
    "# enforce schema per-file\n",
    "df = enforce_schema(df, SCHEMA, strict=False)   # start with strict=False while you’re still harmonizing\n",
    "\n",
    "df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = enforce_schema(df, SCHEMA, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c925853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_report(df: pd.DataFrame, schema: dict) -> None:\n",
    "    expected = set(schema.keys())\n",
    "    got = set(df.columns)\n",
    "    missing = sorted(expected - got)\n",
    "    extra = sorted(got - expected)\n",
    "    if missing: print(\"Missing:\", missing)\n",
    "    if extra: print(\"Extra:\", extra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_report(df, SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c93109",
   "metadata": {},
   "source": [
    "**Truth In Sentencing**\n",
    "\n",
    "Clean up the mix of floats and strings, percents and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6406c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tis = (\n",
    "    df[\"truth_in_sentencing\"]\n",
    "        .astype(\"string\")\n",
    "        .str.strip()\n",
    "        .replace({\"\": pd.NA, \"nan\": pd.NA, \"Missing\": pd.NA})\n",
    ")\n",
    "\n",
    "pct = tis.str.extract(r'(\\d+\\.?\\d*)\\s*%')[0]\n",
    "df[\"tis_pct\"] = pd.to_numeric(pct, errors=\"coerce\") / 100\n",
    "\n",
    "decimal_mask = tis.str.match(r'^0?\\.\\d+$|^1\\.0+$|^1$')\n",
    "df.loc[decimal_mask, \"tis_pct\"] = pd.to_numeric(tis[decimal_mask], errors=\"coerce\")\n",
    "\n",
    "df.loc[tis.str.contains(\"day\", case=False, na=False), \"tis_pct\"] = 0.5\n",
    "\n",
    "df[\"tis_desc\"] = (\n",
    "    tis\n",
    "      .str.replace(r'^\\d+\\.?\\d*\\s*%', '', regex=True)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "df.loc[df[\"tis_desc\"] == \"\", \"tis_desc\"] = pd.NA\n",
    "\n",
    "life_mask = df[\"sentence_years\"].isin([\"LIFE\", \"SDP\"])\n",
    "df.loc[life_mask, [\"tis_pct\", \"tis_desc\"]] = pd.NA\n",
    "\n",
    "df[\"tis_pct\"] = df[\"tis_pct\"].astype(\"float32\")\n",
    "df[\"tis_desc\"] = df[\"tis_desc\"].astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1481b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tis_pct\"].value_counts().head(10)\n",
    "df[\"truth_in_sentencing\"] = tis.astype(\"category\")\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22e850",
   "metadata": {},
   "source": [
    "**Sentence Length**\n",
    "\n",
    "* The formatting of the sentence years and months is inconsistent.\n",
    "* Create columns showing the total sentencing length in months and in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0292f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SPECIAL = {\"LIFE\", \"SDP\", \"DEATH\", \"Unknown\"}\n",
    "\n",
    "# --- normalize raw strings (years + months) ---\n",
    "def _norm_series(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(\"string\")\n",
    "         .str.strip()\n",
    "         .replace({\"\": pd.NA, \"nan\": pd.NA, \" \": pd.NA})\n",
    "    )\n",
    "\n",
    "df[\"sentence_years_raw\"] = _norm_series(df[\"sentence_years\"])\n",
    "df[\"sentence_months_raw\"] = _norm_series(df[\"sentence_months\"])\n",
    "\n",
    "# years: fix typos + unknowns\n",
    "df[\"sentence_years_raw\"] = df[\"sentence_years_raw\"].replace({\n",
    "    \"LFE\": \"LIFE\",\n",
    "    \"PEND\": \"Unknown\",\n",
    "})\n",
    "df[\"sentence_years_raw\"] = df[\"sentence_years_raw\"].fillna(\"Unknown\")\n",
    "\n",
    "# months: if years is special, force months to same special label\n",
    "is_special_year = df[\"sentence_years_raw\"].isin(SPECIAL)\n",
    "df.loc[is_special_year, \"sentence_months_raw\"] = df.loc[is_special_year, \"sentence_years_raw\"]\n",
    "\n",
    "# months: unknown handling for non-special\n",
    "df.loc[~is_special_year, \"sentence_months_raw\"] = df.loc[~is_special_year, \"sentence_months_raw\"].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# --- parse numeric years/months where possible ---\n",
    "years_num = pd.to_numeric(df[\"sentence_years_raw\"], errors=\"coerce\")\n",
    "months_num = pd.to_numeric(df[\"sentence_months_raw\"], errors=\"coerce\")\n",
    "\n",
    "# where years are numeric AND months numeric, keep them\n",
    "is_numeric = years_num.notna() & months_num.notna()\n",
    "\n",
    "# fix cases:\n",
    "# (A) if years > 0 and months >= 12 -> set months = 0 (trust years)\n",
    "fix_months_gt12 = is_numeric & (years_num > 0) & (months_num >= 12)\n",
    "months_num = months_num.mask(fix_months_gt12, 0)\n",
    "\n",
    "# (B) if years == 0 and months >= 12 -> convert months to years+months\n",
    "fix_years0_months_ge12 = is_numeric & (years_num == 0) & (months_num >= 12)\n",
    "years_num = years_num.mask(fix_years0_months_ge12, (months_num // 12).astype(\"Int64\"))\n",
    "months_num = months_num.mask(fix_years0_months_ge12, (months_num % 12).astype(\"Int64\"))\n",
    "\n",
    "# store cleaned numeric versions (nullable ints)\n",
    "df[\"sentence_years_int\"] = years_num.round().astype(\"Int64\")\n",
    "df[\"sentence_months_int\"] = months_num.round().astype(\"Int64\")\n",
    "\n",
    "\n",
    "# --- build analysis variables ---\n",
    "# total months numeric (nullable)\n",
    "df[\"sentence_total_months\"] = (df[\"sentence_years_int\"] * 12 + df[\"sentence_months_int\"]).astype(\"Int64\")\n",
    "\n",
    "# continuous years numeric (float) where numeric\n",
    "df[\"sentence_years_cont\"] = (df[\"sentence_total_months\"] / 12).astype(\"Float32\")\n",
    "\n",
    "# keep a clean label version for special sentences\n",
    "df[\"sentence_years_label\"] = df[\"sentence_years_raw\"].where(df[\"sentence_years_raw\"].isin(SPECIAL), pd.NA)\n",
    "df[\"sentence_years_label\"] = df[\"sentence_years_label\"].astype(\"category\")\n",
    "\n",
    "# optional: drop raw helper cols after QA\n",
    "# df = df.drop(columns=[\"sentence_years_raw\", \"sentence_months_raw\"])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df[\"sentence_months_int\"].notna(), \"sentence_months_int\"].describe())\n",
    "\n",
    "mask = df[\"sentence_months_int\"].notna() & (df[\"sentence_months_int\"] >= 12)\n",
    "print(df.loc[mask, [\"sentence_years_int\", \"sentence_months_int\"]].head())\n",
    "\n",
    "print(df[\"sentence_years_label\"].value_counts(dropna=False))\n",
    "print(\"numeric rows:\", df[\"sentence_total_months\"].notna().sum(), \"of\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310d0cb",
   "metadata": {},
   "source": [
    "**Reformat dates**\n",
    "\n",
    "Some dates are saved as strings. These can be in various formats. They need to all be converted to dates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17927e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what different formats can the dates take?\n",
    "# (add len col and str col for each date col)\n",
    "\n",
    "date_cols = [c for c in df.columns if \"date\" in c.lower()]\n",
    "\n",
    "for col in date_cols:\n",
    "    print(\"\\n\", col)\n",
    "\n",
    "    lens = (\n",
    "        df[col]\n",
    "        .astype(\"string\")\n",
    "        .str.len()\n",
    "        .value_counts(dropna=False)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    print(lens)\n",
    "date_cols = [\n",
    "    \"date_of_birth\",\n",
    "    \"current_admission_date\",\n",
    "    \"custody_date\",\n",
    "    \"sentence_date\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date\",\n",
    "    \"projected_discharge_date\",\n",
    "    \"doc_date\",\n",
    "]\n",
    "\n",
    "for c in date_cols:\n",
    "    # ensure datetime first, then cast to microseconds\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\").values.astype(\"datetime64[us]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9010bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in date_cols:\n",
    "    print(c, df[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce12aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# play around with this code to see what the various lengths look like\n",
    "dob_s = df[\"date_of_birth\"].astype(\"string\")\n",
    "adm_s = df[\"current_admission_date\"].astype(\"string\")\n",
    "cus_s = df[\"custody_date\"].astype(\"string\")\n",
    "doc_s = df[\"doc_date\"].astype(\"string\")\n",
    "id_s  = df[\"idoc_#\"].astype(\"string\")\n",
    "\n",
    "\n",
    "#Show me examples where admission date string length == 10\n",
    "mask = adm_s.str.len().eq(10)\n",
    "print(df.loc[mask, [\"current_admission_date\"]].head(10))\n",
    "\n",
    "#Check for a period in DOB where string length == 8\n",
    "mask = dob_s.str.contains(r\"\\.\", regex=True, na=False) & dob_s.str.len().eq(8)\n",
    "print(df.loc[mask, [\"date_of_birth\", \"idoc_#\", \"doc_date\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the actual unparsed / weird values\n",
    "\n",
    "for col in [\"date_of_birth\", \"current_admission_date\", \"custody_date\", \"sentence_date\"]:\n",
    "    s = df[col].astype(\"string\").str.strip()\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    bad = df.loc[parsed.isna() & s.notna(), col].astype(\"string\").dropna().unique()[:20]\n",
    "    print(\"\\n\", col, \"examples of problematic values:\")\n",
    "    print(bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c41cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert to dates\n",
    "# (using None instead of np.nan so that string length can still be calculated)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SENTINELS = {\n",
    "    \"00000000\", \"0 0 0 0\", \" 0000000\",\n",
    "    \"99999999\", \"99959999\", \"99989999\",\n",
    "    \"82220288\",\n",
    "}\n",
    "\n",
    "def clean_date_column(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Clean a messy date column into pandas datetime (keeps out-of-bounds years by staying at us resolution).\n",
    "    Handles:\n",
    "      - ISO (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS)\n",
    "      - MMDDYYYY (8 digits)\n",
    "      - MDDYYYY (7 digits)\n",
    "      - trailing '.' and whitespace\n",
    "      - sentinel values (00000000, 99999999, etc.)\n",
    "      - very short junk (len <= 3)\n",
    "    \"\"\"\n",
    "    # work as string for cleaning\n",
    "    x = s.astype(\"string\").str.strip()\n",
    "\n",
    "    # blank / short junk -> NA\n",
    "    x = x.mask(x.isna() | (x.str.len() <= 3), pd.NA)\n",
    "\n",
    "    # strip a trailing period\n",
    "    x = x.str.replace(r\"\\.$\", \"\", regex=True)\n",
    "\n",
    "    # null out any strings containing a period anywhere else (your old rule)\n",
    "    x = x.mask(x.str.contains(r\"\\.\", regex=True, na=False), pd.NA)\n",
    "\n",
    "    # remove non-digit separators for numeric formats, but keep ISO intact\n",
    "    # (we'll parse ISO separately first)\n",
    "    iso = x.where(x.str.match(r\"^\\d{4}-\\d{2}-\\d{2}\", na=False), pd.NA)\n",
    "    non_iso = x.where(~x.str.match(r\"^\\d{4}-\\d{2}-\\d{2}\", na=False), pd.NA)\n",
    "\n",
    "    # sentinel codes (on the non-iso side)\n",
    "    non_iso = non_iso.mask(non_iso.isin(SENTINELS), pd.NA)\n",
    "\n",
    "    # keep only digits for numeric date formats (e.g. 01022020, 1022020)\n",
    "    non_iso_digits = non_iso.str.replace(r\"\\D\", \"\", regex=True)\n",
    "\n",
    "    # 7-digit -> pad to 8 (MDDYYYY -> 0MDDYYYY)\n",
    "    non_iso_digits = non_iso_digits.mask(non_iso_digits.str.len().eq(7), \"0\" + non_iso_digits)\n",
    "\n",
    "    # Now parse:\n",
    "    # ISO: just take first 10 chars as date\n",
    "    parsed_iso = pd.to_datetime(iso.str.slice(0, 10), errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "\n",
    "    # Numeric: MMDDYYYY\n",
    "    parsed_num = pd.to_datetime(non_iso_digits, errors=\"coerce\", format=\"%m%d%Y\")\n",
    "\n",
    "    # Combine: prefer ISO where available, else numeric\n",
    "    out = parsed_iso.fillna(parsed_num)\n",
    "\n",
    "    # IMPORTANT: keep microsecond resolution so far-future dates can exist (no ns casting)\n",
    "    # Convert to numpy datetime64[us] array (safe)\n",
    "    out = pd.to_datetime(out, errors=\"coerce\").values.astype(\"datetime64[us]\")\n",
    "\n",
    "    return pd.Series(out, index=s.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the timestamps and dates to datetimes\n",
    "date_cols = [c for c in df.columns if \"date\" in c.lower() and c != \"doc_date\"]\n",
    "\n",
    "for c in date_cols:\n",
    "    print(\"\\nCleaning:\", c)\n",
    "    before_missing = df[c].isna().mean()\n",
    "\n",
    "    df[c] = clean_date_column(df[c])\n",
    "\n",
    "    after_missing = pd.isna(df[c]).mean()\n",
    "    print(\"missing before:\", round(before_missing, 4), \"after:\", round(after_missing, 4))\n",
    "    print(\"dtype:\", df[c].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1ff59",
   "metadata": {},
   "source": [
    "**Missing/incorrect birth dates**\n",
    "\n",
    "Some birth dates are missing or typoed. Replace with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_conflicts = (\n",
    "    df.groupby(\"idoc_#\")[\"date_of_birth\"]\n",
    "      .nunique()\n",
    "      .loc[lambda s: s > 1]\n",
    "      .index\n",
    ")\n",
    "\n",
    "conflict_df = (\n",
    "    df.loc[df[\"idoc_#\"].isin(dob_conflicts), [\"idoc_#\", \"date_of_birth\"]]\n",
    "      .drop_duplicates()\n",
    "      .sort_values(\"idoc_#\")\n",
    ")\n",
    "\n",
    "print(conflict_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dob_conflicts) / df[\"idoc_#\"].nunique())\n",
    "\n",
    "(\n",
    "    df.loc[df[\"idoc_#\"].isin(dob_conflicts)]\n",
    "      .groupby(\"idoc_#\")[\"date_of_birth\"]\n",
    "      .agg([\"min\", \"max\", \"nunique\"])\n",
    "      .head(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ddad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many IDs have >1 DOB (before)\n",
    "dob_nunique = df.groupby(\"idoc_#\")[\"date_of_birth\"].nunique(dropna=True)\n",
    "print(\"Original conflicts:\", (dob_nunique > 1).sum())\n",
    "print(\"Original missing DOB rows:\", df[\"date_of_birth\"].isna().sum())\n",
    "\n",
    "# compute DOB mode per person (ties broken deterministically by earliest date)\n",
    "def dob_mode(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return pd.NaT\n",
    "    vc = s.value_counts()\n",
    "    top = vc[vc == vc.max()].index\n",
    "    return min(top)  # tie-break: smallest/earliest\n",
    "\n",
    "dob_mode_map = df.groupby(\"idoc_#\")[\"date_of_birth\"].agg(dob_mode)\n",
    "\n",
    "# overwrite DOB with per-person mode (preserves row count/order)\n",
    "df[\"date_of_birth\"] = df[\"idoc_#\"].map(dob_mode_map)\n",
    "\n",
    "# how many IDs have >1 DOB (after)\n",
    "dob_nunique2 = df.groupby(\"idoc_#\")[\"date_of_birth\"].nunique(dropna=True)\n",
    "print(\"\\nNew conflicts:\", (dob_nunique2 > 1).sum())\n",
    "print(\"New missing DOB rows:\", df[\"date_of_birth\"].isna().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cb5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2e7ee5",
   "metadata": {},
   "source": [
    "**Add Date Diffs**\n",
    "* Age\n",
    "* Age at Custody\n",
    "* Projected Time Until Release\n",
    "* Projected Age at Release\n",
    "\n",
    "(Note: Doing this in Tableau instead rounds to the nearest number, which produces unintuitive results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap the projected dates at 2261; otherwise, element-wise operators won't work\n",
    "\n",
    "NS_MAX = pd.Timestamp(\"2262-04-11\")\n",
    "\n",
    "for col in [\n",
    "    \"projected_discharge_date\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date\"\n",
    "]:\n",
    "    df[col + \"_capped\"] = df[col].clip(upper=NS_MAX)\n",
    "for col in [\n",
    "    \"projected_discharge_date_capped\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date_capped\"\n",
    "]:\n",
    "    df[col] = df[col].astype(\"datetime64[ns]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a204d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get age and age at custody as int\n",
    "\n",
    "df[\"age\"] = (\n",
    "    (df[\"doc_date\"] - df[\"date_of_birth\"])\n",
    "    .dt.days\n",
    "    .floordiv(365.25)\n",
    "    .astype(\"Int16\")\n",
    ")\n",
    "\n",
    "df[\"age_at_custody\"] = (\n",
    "    (df[\"custody_date\"] - df[\"date_of_birth\"])\n",
    "    .dt.days\n",
    "    .floordiv(365.25)\n",
    "    .astype(\"Int16\")\n",
    ")\n",
    "\n",
    "print(df[\"age\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e556e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking why there is an 8 and 9 year old\n",
    "bad_age = df[(df[\"age\"] < 10) | (df[\"age\"] > 100)]\n",
    "\n",
    "print(len(bad_age))\n",
    "print(\n",
    "    bad_age[\n",
    "        [\"idoc_#\", \"date_of_birth\", \"custody_date\", \"doc_date\", \"age\", \"age_at_custody\", \"crime_class\", \"holding_offense\",\n",
    "\"holding_offense_category\", \"offense_type\"]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for multiple crimes per person\n",
    "multi_crime = (\n",
    "    df.groupby(\"idoc_#\")[\"holding_offense\"]\n",
    "      .nunique()\n",
    "      .gt(1)\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "print(multi_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "    \"date_of_birth\",\n",
    "    \"current_admission_date\",\n",
    "    \"custody_date\",\n",
    "    \"sentence_date\",\n",
    "    \"projected_mandatory_supervised_release_(msr)_date_capped\",\n",
    "    \"projected_discharge_date_capped\",\n",
    "    \"doc_date\",\n",
    "]\n",
    "\n",
    "df3 = df.copy()\n",
    "for c in date_cols:\n",
    "    df3[c] = pd.to_datetime(df3[c], errors=\"coerce\").dt.normalize().astype(\"datetime64[ns]\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b50837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_parquet(\"idoc_clean_timestamps.parquet\", index=False)\n",
    "# df = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_col = \"projected_mandatory_supervised_release_(msr)_date_capped\"\n",
    "dis_col = \"projected_discharge_date_capped\"\n",
    "doc_col = \"doc_date\"\n",
    "dob_col = \"date_of_birth\"\n",
    "\n",
    "# --- helper: coerce to day-level numpy datetime64[D] safely ---\n",
    "def to_day_array(s: pd.Series) -> np.ndarray:\n",
    "    # works for datetime64, python date objects, strings, etc.\n",
    "    return pd.to_datetime(s, errors=\"coerce\").to_numpy(dtype=\"datetime64[D]\")\n",
    "\n",
    "# 1) canonical projected release date (keep as-is; can be datetime or date objects)\n",
    "df[\"projected_release_date_capped\"] = df[msr_col]\n",
    "mask = df[msr_col].isna() | (pd.to_datetime(df[msr_col], errors=\"coerce\") <= pd.to_datetime(df[doc_col], errors=\"coerce\"))\n",
    "df.loc[mask, \"projected_release_date_capped\"] = df.loc[mask, dis_col]\n",
    "\n",
    "# 2) compute age at release using day-resolution arrays (no .dt needed)\n",
    "rel_D = to_day_array(df[\"projected_release_date_capped\"])\n",
    "dob_D = to_day_array(df[dob_col])\n",
    "\n",
    "age_days = (rel_D - dob_D).astype(\"timedelta64[D]\").astype(\"float64\")\n",
    "age_years = np.floor(age_days / 365.25)\n",
    "\n",
    "df[\"projected_age_at_release_capped\"] = pd.Series(age_years, index=df.index).astype(\"Float32\").astype(\"Int16\")\n",
    "\n",
    "# if either date missing => NA\n",
    "missing_age = pd.isna(pd.to_datetime(df[\"projected_release_date_capped\"], errors=\"coerce\")) | pd.isna(pd.to_datetime(df[dob_col], errors=\"coerce\"))\n",
    "df.loc[missing_age, \"projected_age_at_release_capped\"] = pd.NA\n",
    "\n",
    "# 3) time until release (tenths of a year) using day-resolution arrays\n",
    "doc_D = to_day_array(df[doc_col])\n",
    "delta_days = (rel_D - doc_D).astype(\"timedelta64[D]\").astype(\"float64\")\n",
    "\n",
    "x = 10 * (delta_days / 365.0)\n",
    "x_adj = np.where(x < 0, x - 1, x)\n",
    "df[\"projected_time_until_release_capped\"] = (np.trunc(x_adj) / 10).astype(\"float32\")\n",
    "\n",
    "missing_time = pd.isna(pd.to_datetime(df[\"projected_release_date_capped\"], errors=\"coerce\")) | pd.isna(pd.to_datetime(df[doc_col], errors=\"coerce\"))\n",
    "df.loc[missing_time, \"projected_time_until_release_capped\"] = np.nan\n",
    "\n",
    "# 4) quick QA\n",
    "print(df[\"projected_age_at_release_capped\"].describe())\n",
    "print(df[\"projected_time_until_release_capped\"].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73a12b",
   "metadata": {},
   "source": [
    "**Latest record**\n",
    "\n",
    "Since the data is produced quarterly, some individuals will be repeated in the data. Get the date of the most recent document in which each individual appears.\n",
    "\n",
    "Note: This takes a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ed023",
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = len(df)\n",
    "blank_date_df = df[pd.isnull(df['Current Admission Date'])]\n",
    "non_blank_date_df = df[~pd.isnull(df['Current Admission Date'])]\n",
    "\n",
    "non_blank_date_df['Current Admission Date str'] = non_blank_date_df['Current Admission Date'].apply(lambda x: dt.strftime(x, '%Y%m%d'))\n",
    "non_blank_date_df['ID'] = non_blank_date_df['IDOC #'].add(non_blank_date_df['Current Admission Date str'])\n",
    "non_blank_date_df.drop('Current Admission Date str', axis=1, inplace=True)\n",
    "\n",
    "blank_date_df['ID'] = non_blank_date_df['IDOC #']\n",
    "df = pd.concat([blank_date_df, non_blank_date_df])\n",
    "\n",
    "if len(df) != len1:\n",
    "    print(len(df))\n",
    "    print(len1)\n",
    "    print('blanking dates changed the length')\n",
    "    1/0\n",
    "\n",
    "# get most recent doc date\n",
    "doc_date_series = df[['ID', 'Doc Date']]\n",
    "doc_date_series.rename(columns={'Doc Date': 'Last Doc Date'}, inplace=True)\n",
    "doc_date_series = doc_date_series.groupby('ID')['Last Doc Date'].max()\n",
    "df = df.merge(doc_date_series, how='inner', on='ID')\n",
    "\n",
    "# remove unecessary columns\n",
    "for col in df.columns:\n",
    "    if col[-4:] in [' len', ' str'] or col in ['index', 'Idx']:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "# (need to drop Doc Date to remove all duplicates)\n",
    "        \n",
    "# remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# drop ID col\n",
    "df.drop('ID', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec721f38",
   "metadata": {},
   "source": [
    "**Admission Type - Label Consistency**\n",
    "\n",
    "Update labels for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca31908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update to title case\n",
    "df['Admission Type'] = df['Admission Type'].str.title()\n",
    "df['Admission Type'] = df['Admission Type'].str.replace('Covid', 'COVID').str.replace('Msr', 'MSR').str.replace('Idoc', 'IDOC').str.replace('Edv', 'EDV')\n",
    "\n",
    "# check the values\n",
    "pd.set_option('display.max_rows', 60)\n",
    "print(df['Admission Type'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# update values for consistency\n",
    "df.loc[df['Admission Type'] == 'Conditional Release, New Sent', 'Admission Type'] = 'Conditional Release, New Sentence'\n",
    "df.loc[df['Admission Type'] == 'Work Release Violator, New Sentnce', 'Admission Type'] = 'Work Release Violator, New Sentence'\n",
    "\n",
    "# replace commas with hyphens\n",
    "df['Admission Type'] = df['Admission Type'].str.replace(',', ' -')\n",
    "\n",
    "# replace None with nan\n",
    "df.loc[df['Admission Type'] == 'None', 'Admission Type'] = np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce02ea4",
   "metadata": {},
   "source": [
    "**Admission Type - Changed Labels**\n",
    "\n",
    "IDOC changed their labels during certain quarters in more recent data. Check which changed and categorize appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df = df[df['Doc Date'].apply(lambda x: x.date()) >= date(2020, 1, 1)]\n",
    "\n",
    "# for recent data, plot the number of individuals with each admission type\n",
    "plot_df = change_df[['Admission Type', 'Doc Date', 'IDOC #']].groupby(['Admission Type', 'Doc Date']).count().reset_index()\n",
    "\n",
    "# keep only the admission types with significant numbers\n",
    "plot_df = plot_df[plot_df['IDOC #'] > 10]\n",
    "\n",
    "# convert dates to strings\n",
    "plot_df['Doc Date'] = plot_df['Doc Date'].apply(lambda x: dt.strftime(x, '%Y-%m-%d'))\n",
    "\n",
    "# get doc_dates for x axis\n",
    "doc_dates = plot_df[['Doc Date']].drop_duplicates()\n",
    "\n",
    "# plot bar charts for admission types each quarter\n",
    "fig, axs = plt.subplots(len(plot_df['Admission Type'].unique()), figsize=(8, 50), sharex=True)#, sharey=True)\n",
    "i = 0\n",
    "\n",
    "for at in plot_df['Admission Type'].unique():\n",
    "    # get the relevant df ordered by doc date\n",
    "    at_df = plot_df[plot_df['Admission Type'] == at]\n",
    "    at_df = pd.merge(doc_dates, at_df, on='Doc Date', how='left')\n",
    "    date_order = sorted(doc_dates['Doc Date'].unique())\n",
    "    at_df = at_df.set_index(at_df['Doc Date']).loc[date_order]\n",
    "    \n",
    "    # plot the relevant df\n",
    "    axs[i].bar(at_df['Doc Date'], at_df['IDOC #'])\n",
    "    axs[i].title.set_text(at)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc126a25",
   "metadata": {},
   "source": [
    "For Jun 2021 going forward, IDOC used different labels.\n",
    "* Before Jun 2021\n",
    "    * Direct From Court\n",
    "    * Admit From Other Custody\n",
    "    * MSR Violator - New Sentence\n",
    "    * Parole Violator - New Sentence\n",
    "    * Discharged & Recommitted\n",
    "    * Conditional Release Violator\n",
    "    * Technical MSR Violator\n",
    "    * Technical Parole Violator\n",
    "    * Transferred from Juvenile\n",
    "    * Return Additional Mittimus\n",
    "    * COVID-19 County Admit\n",
    "* Jun 2021 forward\n",
    "    * Court Admissions\n",
    "    * New Sentence Violators\n",
    "    * Technical Violators\n",
    "    * Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for a sankey diagram to show the relationship between the labels in the two time periods\n",
    "\n",
    "sankey_df = df[(df['Doc Date'].apply(lambda x: x.date()) >= date(2020, 3, 1)) & (df['Doc Date'].apply(lambda x: x.date()) <= date(2022, 9, 30))]\n",
    "sankey_df = sankey_df[sankey_df['Admission Type'] != 'None']\n",
    "\n",
    "# convert dates to strings\n",
    "sankey_df['Doc Date'] = sankey_df['Doc Date'].apply(lambda x: dt.strftime(x, '%Y-%m-%d'))\n",
    "\n",
    "# break into before and after\n",
    "sankey_df = sankey_df[['IDOC #', 'Current Admission Date', 'Admission Type', 'Doc Date']].drop_duplicates()\n",
    "after_df = sankey_df[sankey_df['Doc Date'].apply(lambda x: dt.strptime(x, '%Y-%m-%d').date()) >= date(2021, 6, 1)]\n",
    "before_df = sankey_df[~sankey_df.index.isin(after_df.index)]\n",
    "\n",
    "# recombine with before and after columns\n",
    "after_df = after_df.drop('Doc Date', axis=1).rename(columns={'Admission Type': 'After Admission Type'})\n",
    "before_df = before_df.drop('Doc Date', axis=1).rename(columns={'Admission Type': 'Before Admission Type'})\n",
    "sankey_df = pd.merge(before_df, after_df, on=['IDOC #', 'Current Admission Date'], how='outer')\n",
    "\n",
    "# drop dupes and count totals for each before/after combo\n",
    "sankey_df = sankey_df.drop_duplicates()[['Before Admission Type', 'After Admission Type', 'IDOC #']].groupby(['Before Admission Type', 'After Admission Type']).count().reset_index().rename(columns={'IDOC #': 'Count'})\n",
    "print(sankey_df)\n",
    "\n",
    "# get the node df - a df dictionary of admission types and indices\n",
    "node_df = pd.concat([sankey_df[['Before Admission Type']].rename(columns={'Before Admission Type': 'AT'}),\n",
    "                       sankey_df[['After Admission Type']].rename(columns={'After Admission Type': 'AT'})])\n",
    "node_df = node_df.drop_duplicates().reset_index(drop=True).reset_index().rename(columns={'index': 'ID'})\n",
    "node_df['Color'] = get_hex(len(node_df))\n",
    "\n",
    "# get the links df\n",
    "links_df = pd.merge(sankey_df, \n",
    "                    node_df[['AT', 'ID', 'Color']].rename(columns={'ID': 'Source', 'Color': 'Link Color'}), \n",
    "                    left_on='Before Admission Type', right_on='AT', how='inner')\n",
    "links_df = pd.merge(links_df, \n",
    "                    node_df[['AT', 'ID']].rename(columns={'ID': 'Target'}), \n",
    "                    left_on='After Admission Type', right_on='AT', how='inner')\n",
    "links_df['Link Color'] = links_df['Link Color'].apply(lambda x: 'rgba' + str(get_rgb_from_hex(x) + (0.5,)))\n",
    "links_df = links_df[['Source', 'Target', 'Link Color', 'Count']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252dacac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 20,\n",
    "        thickness = 30,\n",
    "        line = dict(\n",
    "            color = 'black',\n",
    "            width = 0\n",
    "        ),\n",
    "        label = node_df['AT'],\n",
    "        color = node_df['Color']\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = links_df['Source'],\n",
    "        target = links_df['Target'],\n",
    "        value = links_df['Count'],\n",
    "        color = links_df['Link Color']\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text='Change in Admission Type in Jun and Dec 2021',\n",
    "                  height=1000,\n",
    "                  font_size=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc60cf4",
   "metadata": {},
   "source": [
    "For the most part, the new admission types group multiple old admission types.\n",
    "* Court Admissions\n",
    "    * Direct From Court\n",
    "    * Discharged & Recommitted\n",
    "    * Return Additional Mittimus\n",
    "    * COVID-19 County Admit (but let's keep this as its own since its timeframe is so limited)\n",
    "    * Transferred From Juvenile\n",
    "* Other\n",
    "    * Admit from Other Custody\n",
    "    * Conditional Release Violator\n",
    "    * Not In IDOC Custody\n",
    "    * Return To Custody\n",
    "* New Sentence Violators\n",
    "    * MSR Violator - New Sentence\n",
    "    * Parole Violator - New Sentence\n",
    "* Technical Violators\n",
    "    * Technical MSR Violator\n",
    "    * Technical Parole Violator\n",
    "    \n",
    "Let's create a new field with the new admission types grouping the old admission types. Group others (from past quarters) based on similarity to other old admission types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_admis_list = ['Direct From Court', 'Discharged & Recommitted', 'Return Additional Mittimus', 'Transferred From Juvenile']\n",
    "other_list = ['Admit From Other Custody', 'Conditional Release Violator', 'Not In IDOC Custody', 'Return To Custody', 'Work Release Violator', 'EDV', 'Juv Parole To Adult Parole']\n",
    "new_sent_viol_list = ['MSR Violator - New Sentence', 'Parole Violator - New Sentence', 'Conditional Release - New Sentence', 'Work Release Violator - New Sentence']\n",
    "tech_viol_list = ['Technical MSR Violator', 'Technical Parole Violator']\n",
    "\n",
    "df['New Admission Type'] = None\n",
    "df.loc[df['Admission Type'].isin(ct_admis_list), 'New Admission Type'] = 'Court Admissions'\n",
    "df.loc[df['Admission Type'].isin(other_list), 'New Admission Type'] = 'Other'\n",
    "df.loc[df['Admission Type'].isin(new_sent_viol_list), 'New Admission Type'] = 'New Sentence Violators'\n",
    "df.loc[df['Admission Type'].isin(tech_viol_list), 'New Admission Type'] = 'Technical Violators'\n",
    "\n",
    "df.loc[pd.isnull(df['New Admission Type']), 'New Admission Type'] = df.loc[pd.isnull(df['New Admission Type']), 'Admission Type']\n",
    "\n",
    "print(df['New Admission Type'].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938509e",
   "metadata": {},
   "source": [
    "**Admission Type - Categorization**\n",
    "\n",
    "Bucket into first-time vs. second-time vs. technical offender. \n",
    "\n",
    "NOTE: This is outdated and the logic cannot be used with new IDOC admission types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list = ['DIRECT FROM COURT', 'TRANSFERRED FROM JUVENILE', 'JUV PAROLE TO ADULT PAROLE']\n",
    "second_list = ['DISCHARGED & RECOMMITTED', 'MSR VIOLATOR - NEW SENTENCE', 'PAROLE VIOLATOR - NEW SENTENCE', 'WORK RELEASE VIOLATOR - NEW SENTENCE', 'CONDITIONAL RELEASE - NEW SENTENCE']\n",
    "tech_list = ['TECHNICAL PAROLE VIOLATOR', 'TECHNICAL MSR VIOLATOR', 'CONDITIONAL RELEASE VIOLATOR', 'TECHNICAL VIOLATORS', 'WORK RELEASE VIOLATOR']\n",
    "\n",
    "df['Admission Type - Recidivism'] = None\n",
    "df.loc[df['Admission Type'].str.upper().isin(first_list), 'Admission Type - Recidivism'] = 'First IDOC Sentence'\n",
    "df.loc[df['Admission Type'].str.upper().isin(second_list), 'Admission Type - Recidivism'] = 'Second/+ IDOC Sentence'\n",
    "df.loc[df['Admission Type'].str.upper().isin(tech_list), 'Admission Type - Recidivism'] = 'Technical MSR/Parole Violator'\n",
    "\n",
    "df.loc[pd.isnull(df['Admission Type - Recidivism']), 'Admission Type - Recidivism'] = 'Unknown/Uncategorized'\n",
    "\n",
    "print(df['Admission Type - Recidivism'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec5ddf",
   "metadata": {},
   "source": [
    "**Holding offenses**\n",
    "\n",
    "There are a lot of similar but slightly different values here. They need to be bucketed/categorized.\n",
    "* Some can be bucketed based on the JHA interns' list.\n",
    "* For the rest, bucket based on keywords found in the holding offense.\n",
    "    * The order of operations matters. E.g. GUN --> weapon offense must come before POSS --> drug offense, since many weapons offenses include the string POSS.\n",
    "* There is already a Holding Offense Category field (which is very specific), and an Offense Type field (which is broad), but neither are available before Sep 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcb01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['Holding Offense Category'].value_counts(dropna=False))\n",
    "print(df['Offense Type'].value_counts(dropna=False))\n",
    "\n",
    "len1 = len(df)\n",
    "\n",
    "df['Holding Offense'] = df['Holding Offense'].apply(lambda x: str(x).strip())\n",
    "\n",
    "\n",
    "# specific holding offenses from interns' list\n",
    "inj_death_list = ['AB/NEGLECT ELD/DEATH/CAREGIVER', 'AGG BATTERY CHILD <13/GREAT BOD HARM', 'AGG BATTERY OF A CHILD', 'AGG BATTERY W/FIREARM/PERSON', 'AGG BATTERY/CHILD <13/PERM DISABL', 'AGG BATTERY/DISCHARGE FIREARM', 'AGG BATTERY/FIREARM/OFF/FIREMAN', 'AGG BATTERY/GREAT BOD HARM/60+', 'AGG BATTERY/GREAT BOD HARM/FLAME SUBST', 'AGG BATTERY/GREAT BOD HARM/PC OFF', 'AGG BATTERY/GREAT BODILY HARM', 'AGG BATTERY/HARM/PEACE OFFICER', 'AGG BATTERY/MERCHANT', 'AGG BTRY/UNLAWFUL DEL/CON SUB', 'AGG KIDNAPING/INFLICT HARM', 'AGG STALKING/BODILY HARM', 'AGG STALKING/BODILY HARM/2ND', 'AGG VEH HIJACKING/DISCH/HARM', 'AGGRAVATED BATTERY W/FIREARM', 'ARMED ROBBERY/DISCHARGE/HARM', 'DOM BTRY/BOD HARM/4+ PRI CONV', 'DOMESTIC BATTERY/BODILY HARM', 'HEINOUS BATTERY', 'HOME INVASION/CAUSE INJURY', 'HOME INVASION/DISCH FIREARM/HARM', 'INTIMIDATION/PHYSICAL HARM', 'MURDER/2ND DEGREE MURDER', 'MURDER/2ND DEGREE/PROVOCATION', 'MURDER/2ND DEGREE/UNREASON', 'MURDER/HOMICIDE/UNBORN CHILD', 'MURDER/INTENT TO KILL/INJURE', 'MURDER/OTHER FORCIBLE FELONY', 'MURDER/STRONG PROB KILL/INJURE', 'RECKLESS CONDUCT/GREAT BODILY HARM', 'VOLUN MANSL/NEGL-DEATH ANOTHER', 'DRUG INDUCED HOMICIDE', 'AGG DUI BODILY HARM/.10+/CHILD', 'AGG DUI/ACCIDENT/BODILY HARM', 'AGG DUI/ACCIDENT/DEATH', 'AGG DUI/BODILY HARM W/CHILD', 'AGG DUI/DEATH OF ANOTHER', 'AGG DUI/GREAT BODILY HARM', 'INVOL MANSLAUGHTER ATV/BOAT', 'INVOL MANSLAUGHTER FAMILY/BOAT', 'RECKLESS HOMIC/INCLINE/DEATH 2+', 'RECKLESS HOMIC/TRAFFIC CONTROL', 'RECKLESS HOMICIDE', 'RECKLESS HOMICIDE/.08 ALC/DRUG', 'RECKLESS HOMICIDE/ALC/CONVIC', 'RECKLESS HOMICIDE/ALC/DRUG', 'RECKLESS HOMICIDE/KILL 2 INDIV', 'SNOWMB OUI OF ALCOHOL/DEATH', 'INVOL MANSLAUGHTER/FAMILY MEMBER', 'INVOLUNTARY MANSLAUGHTER']\n",
    "inj_death_unclear_list = ['AGG ASLT PEACE OFF/FIRE/ER WRK', 'AGG ASLT/OP MOTOR VEH/STRUCK', 'AGG ASLT/STATE IL EMP/WEAPON', 'AGG ASSAULT/CORRECTIONAL EMP', 'AGG ASSAULT/DHS EMPLOYEE', 'AGG ASSAULT/DISCH FIREARM MV', 'AGG ASSAULT/OP MOTOR VEH/PC OFF', 'AGG ASSAULT/USE FOREARM/PEACE OFF', 'AGG BATTERY SR CITIZEN >60 YRS', 'AGG BATTERY/CONTROLLED SUB', 'AGG BATTERY/GOVERNMENT EMP', 'AGG BATTERY/INGEST TOXIC SUBST', 'AGG BATTERY/JUDGE/EMT', 'AGG BATTERY/PEACE OFF/FIREMAN', 'AGG BATTERY/PEACE OFFICER', 'AGG BATTERY/PREGNANT/HANDICAPPED', 'AGG BATTERY/PUBLIC PLACE', 'AGG BATTERY/STRANGLE/PREV CONV', 'AGG BATTERY/TRANSIT EMPLOYEE', 'AGG BATTERY/USE DEADLY WEAPON', 'AGG BATTERY/WEAPON/NO FIREARM', 'AGG DISCH FIREARM/PC OFF/FIREMAN', 'AGG DISCH FIREARM/VEH/PC OFF/FRMAN', 'AGG DISCH SILENCER PERSON/VEH', 'AGG DISCHARGE FIREARM/BLDG', 'AGG DISCHARGE FIREARM/OCC BLDG', 'AGG DISCHARGE FIREARM/OCC VEH', 'AGG DOMESTIC BATTERY/STRANGLE', 'AGG INTIMIDATION PEACE OFFICER', 'AGG KIDNAPING ARMED W FIREARM', 'AGG KIDNAPING/CHILD<13/RETARDED', 'AGG KIDNAPING/CONCEAL IDENT', 'AGG ROBBERY/CONTROLLED SUB', 'AGG ROBBERY/INDICATE ARM W/FIR', 'AGG STALKING/VIO REST ORDER', 'AGG VEHICULAR HIJACKING/DISCHARGE', 'AGG VEHICULAR HIJACKING/FIREARM', 'AGG VEHICULAR HIJACKING/HANDICAPPED', 'AGG VEHICULAR HIJACKING/PASS <16 YRS', 'AGG VEHICULAR HIJACKING/WEAPON', 'AGGRAVATED ASSAULT', 'AGGRAVATED BATTERY/NURSE', 'AGGRAVATED BATTERY/STRANGLE', 'AGGRAVATED BATTERY/VICTIM 60+', 'AGGRAVATED DISCHARGE/FIREARM', 'AGGRAVATED DOMESTIC BATTERY', 'AGGRAVATED KIDNAPING/ARMED', 'AGGRAVATED KIDNAPING/RANSOM', 'AGGRAVATED ROBBERY', 'AGGRAVATED TRAFFICKING/BENEFIT', 'AGGRAVATED UNLAWFUL RESTRAINT', 'AGGRAVATED VEHICULAR HIJACKING', 'ARMED ROBBERY', 'ARMED ROBBERY/ARMED W/FIREARM', 'ARMED ROBBERY/DISCH FIREARM', 'ARMED ROBBERY/NO FIREARM', 'ARMED VIOL/CATEGORY I WEAPON', 'ARMED VIOL/CATEGORY II WEAP/1ST', 'ARMED VIOL/CATEGORY II WEAPON', 'ARMED VIOL/CATEGORY III WEAPON', 'CAUSE CHILD/ENDANGERED/2+', 'CHILD ABDUCTN/CONCEAL/DETAIN', 'COMMUNICATE/DETAIN WITNESS', 'CRIM TRES TO RES/PERS PRESENT', 'DISMEMBERING A HUMAN BODY', 'DOM BTRY/CONTACT/3 PRIOR CONV', 'DOMESTIC BATTERY/ CONTACT/PRIOR', 'DOMESTIC BATTERY/PHY CONTACT/2ND+', 'DOMESTIC BATTERY/VIOL PROTECT ORDER', 'GIVE FALSE BOMB/GAS ALARM', 'HABITUAL CRIMINAL/2 PRIOR CL X', 'HARASS JURORS/WITNESSES', 'HARASS WITNESS', 'HARASSMENT BY PHONE/4TH +', 'HOME INVASION', 'HOME INVASION/ARMED W/FIREARM', 'HOME INVASION/ARMED/FORCE', 'HOME INVASION/DANDEROUS WEAP', 'HOME INVASION/DISCH FIREARM', 'HOME INVASION/FIREARM', 'HOME INVASION/SEX OFFENSE', 'HOME INVASION/VIO AGAINST PERS', 'HRSMT/PREVENT/SERV/SAME VIC', 'HRSMT/THREATEN PERSON/KILL', 'HUMAN TRAFFICKING FOR LABOR/BENEFITS', 'INTIMIDATION/CONTEMPT/RIDICULE', 'INTIMIDATION/CRIMINAL OFFENSE', 'INVOL SERVITUDE/THREAT PHYS HARM', 'KIDNAPING ARMED WITH FIREARM', 'KIDNAPING DISCHARGE FIREARM', 'KIDNAPING W FORCE OR THREAT', 'KIDNAPING/SECRETLY CONFINE', 'PHONE HRSMT/LEWD/THREAT KILL', 'RECKLESS DISCH FIREARM/ENDANGERS', 'ROBBERY', 'ROBBERY/SCHOOL/PLACE WORSHIP', 'ROBBERY/VIC HANDICAP / 60+', 'ROBBERY/VIC HANDICAP OR 60+ YR', 'STALKING BY TRANSMITTING THREAT', 'STALKING/PERSON/SURVEILLANCE', 'STALKING/TRANSMITS THREAT/2ND', 'UNLAWFUL RESTRAINT', 'UNLAWFUL VEHICULAR INVASION', 'VEHICULAR HIJACKING', 'VIO ORDER/PRIOR VIO OF ORDER', 'VIOLATE ORDER/PRIOR DOM BATTERY', 'AB/NEGLECT ELDERLY/CAREGIVER', 'ARMED VIOL/CATEGORY II WEAP/2ND+', 'CONTEMPT', 'KIDNAPING/DECEIT OR ENTICE', 'VIOLATE STALKING NO CONTACT/2+', 'CHILD ABDUCT/LURE/VIC<17/PRIOR', 'WINDOW PEEPING 3RD+', 'ATT AGG BATTERY/CHILD <13/PERM DISABL', 'ATT AGG BATTERY/PEACE OFFICER', 'ATT AGG DISCHARGE FIREARM', 'ATT AGG DISCHARGE FIREARM AT VEH', 'ATT AGG DISCHARGE FIREARM OFFICER', 'ATT AGG VEHICLE HIJACK W WEAPON', 'ATT ARMED ROBBERY/ARMED', 'ATT ARMED ROBBERY/NO FIREARM', 'ATT DRUG-INDUCED HOMICIDE', 'ATT HOME INVASION/ARMED W/FIREARM', 'ATT HOME INVASION/FIREARM', 'ATTEMPT AGG BATTERY OF A CHILD', 'ATTEMPT AGG BATTERY/DISCHARGE FIREARM', 'ATTEMPT AGG DISCHARGE/FIREARM', 'ATTEMPT AGG KIDNAPG/CHILD<13/RETARD', 'ATTEMPT AGG KIDNAPING/INFLICT HARM', 'ATTEMPT AGG ROBBERY/INDICATE ARM W/FIR', 'ATTEMPT AGG VEHICULAR HIJACKING', 'ATTEMPT AGGRAVATED ROBBERY', 'ATTEMPT ARMED ROBBERY', 'ATTEMPT ARMED VIO/CATEGORY I WEAPON', 'ATTEMPT DISARM PC OFF/CORR EMP', 'ATTEMPT HARASS JURORS/WITNESSES', 'ATTEMPT HOME INVASION/CAUSE INJURY', 'ATTEMPT INTENT/HOMICIDE/UNBORN CHILD', 'ATTEMPT KIDNAP ARMED W FIREARM', 'ATTEMPT MURDER/INTENT TO KILL/INJURE', 'ATTEMPT MURDER/OTHER FORCIBLE FELONY', 'ATTEMPT MURDER/STRONG PROB KILL/INJURE', 'ATTEMPT ROBBERY', 'ATTEMPT ROBBERY/VIC HANDICAP OR 60+ YR', 'ATTEMPT SOLICITATION/MURDER/FOR HIRE', 'ATTEMPT VEHICULAR HIJACKING', 'CONSP AGGRAVATED ROBBERY', 'CONSP ARMED ROBBERY', 'CONSP MURDER/INTENT TO KILL/INJURE', 'SOLICIT MURDER/INTENT TO KILL/INJURE', 'SOLICITATION ROBBERY', 'SOLICITATION/MURDER', 'SOLICITATION/MURDER/FOR HIRE', 'CONSP ROBBERY', 'ATTEMPT AGG KIDNAP/CONCEAL IDENTITY', 'ATTEMPT KIDNAPING/SECRETLY CONFINE', 'CONSP AGG BATTERY/DISCHARGE FIREARM']\n",
    "sex_list = ['AGG CHILD PORN/DISSEM FILM', 'AGG CHILD PORN/EXHIBIT', 'AGG CHILD PORN/LEWD EXHIBITION', 'AGG CHILD PORN/PERSON/ANIMAL', 'AGG CHILD PORN/POSS FILM/PHOTO', 'AGG CHILD PORN/POSS FILM/PREV', 'AGG CRIM SEX AB/VIC <18/FAMILY', 'AGG CRIM SEX ABUSE', 'AGG CRIM SEX ABUSE/<5 YR VIC', 'AGG CRIM SEX ABUSE/BODILY HARM', 'AGG CRIM SEX ABUSE/DURING FELONY', 'AGG CRIM SEX ABUSE/FAMILY', 'AGG CRIM SEX ABUSE/FORCE/VIC 9-13', 'AGG CRIM SEX ABUSE/INTELLIG DISABL', 'AGG CRIM SEX ABUSE/VIC <13', 'AGG CRIM SEX ABUSE/VIC 13-16', 'AGG CRIM SEX ABUSE/VIC 13-17', 'AGG CRIM SEX ABUSE/VICTIM <13', 'AGG CRIM SEX ABUSE/VICTIM <9', 'AGG CRIM SEX ASLT/INTEL DISABL', 'AGG CRIM SEX ASLT/THREAT LIFE', 'AGG CRIM SEX ASSAULT/BODILY HARM', 'AGG CRIM SEX ASSAULT/FELONY', 'AGG CRIM SEX ASSAULT/FIREARM', 'AGG CRIM SEX ASSAULT/FORCE VIC9-13', 'AGG CRIM SEX ASSAULT/HANDICAPPED', 'AGG CRIM SEX ASSAULT/RETARDED', 'AGG CRIM SEX ASSAULT/THREAT LIFE', 'AGG CRIM SEX ASSAULT/VIC 60+', 'AGG CRIM SEX ASSAULT/VICTIM <13', 'AGG CRIM SEX ASSAULT/VICTIM <9', 'AGG CRIM SEX ASSAULT/VICTIM >60', 'AGG CRIM SEX ASSAULT/WEAPON', 'AGG CRIM SEXUAL ABUSE/FELONY', 'AGG CRIM SEXUAL ASSAULT/CON SUB', 'AGG CRIM SX AB/VIC 13<18/TRUST', 'AGG CRIM SX ASLT/FORCE VIC9-13', 'AGG IND LIB/CHILD/INFLICT HARM', 'CHILD PORN/FILM/TAPE/PHOTO/ACT', 'CHILD PORN/MOVING DEPICTION', 'CHILD PORN/PERS/ANIM/MOV DPTN', 'CHILD PORN/PERSON/ANIMAL', 'CHILD PORN/POSE/EXHIBITION', 'CHILD PORN/POSS FILM/PHOTOS', 'CHILD PORN/POSS PHOTO/VIC <13', 'CHILD PORN/POSS/MOVING DPTN', 'CHILD PORN/REPRODUCE/MOV DPTN', 'CHILD PORN/REPRODUCE/SELL', 'CHILD PORN/SOL CHILD/MOV DPTN', 'CHILD PORN/SOL/CHILD/APPEAR', 'CHILD PORNOGRAPHY/VICTIM <13', 'CRIM SEX ABUSE/CANT CONSENT/2ND', 'CRIM SEX ASLT/FAM MBR <18/2+', 'CRIM SEX ASLT/FAMILY MEMBER<18', 'CRIM SEX ASLT/FORCE/PREV CONV', 'CRIM SEX ASSAULT/CANT CONSENT', 'CRIM SEX ASSAULT/CANT CONSENT/2ND', 'CRIM SEX ASSAULT/FAMILIES', 'CRIM SEX ASSAULT/FAMILIES/2ND+', 'CRIM SEX ASSAULT/FORCE', 'CRIM SEX ASSAULT/FORCE/2ND+', 'CRIM SEX ASSAULT/SUPERVN VIC 13-17', 'CRIM SEX ASSAULT/VICTIM 13-17', 'CRIM SEX ASSAULT/VICTIM 13-17/2ND', 'CRIM SEXUAL ABUSE/CONSENT', 'CRIM SEXUAL ABUSE/CONSENT/2+', 'CRIMINAL SEX ASSAULT/CONSENT', 'CRIMINAL SEXUAL ABUSE/FORCE', 'DEVIATE SEXUAL ASSAULT', 'GROOMING', 'INDECENT SOL ADULT/<13/PENETRATION', 'INDECENT SOL/AGG CRIM SEX ABUSE', 'INDECENT SOL/CRIM SEX ASSAULT', 'INDECENT SOL/PREDITORY/AGG SEX', 'NONCONSENSUAL  DISSM SEX IMAGE', 'PERMIT SEXUAL ABUSE OF CHILD', 'PRED CRIM SEX ASSLAUT/VICTIM <13', 'PREDATORY CRIM SEX ASSAULT/FIREARM', 'PREDATORY CRIM SEX ASSAULT/HARM', 'PREDATORY CRIMINAL SEXUAL ASSAULT', 'PROM JUV PROST/MINOR <13/RETAR', 'PUBLIC INDECENCY/EXPOSURE/3+', 'RAPE', 'RAPE PRIOR TO 2/1/78', 'SEX RELATIONS WITHIN FAMILIES', 'SEXUALLY DANGEROUS PERSON', 'TRAFFIC SEX SERV MINOR < 17', 'TRAVELING TO MEET A MINOR', 'UNLAWFUL VIDEO/VIC<18/SEX OFF', 'EXPLOIT CHILD/SEX ACT/2ND', 'CUSTODIAL SEXUAL MISCONDUCT', 'INDECENT SOLICIT CHILD/INTERNET', 'ATT AGG CRIM SEX ASSAULT VIC 60+', 'ATT CRIM SEX ASSAULT/FORCE', 'ATT PERMIT SEXUAL ABUSE OF CHILD', 'ATTEMPT AGG CRIM SEX ASLT/BODILY HARM', 'ATTEMPT AGG CRIM SEX ASSAULT', 'ATTEMPT AGG CRIM SEX ASSAULT/BODILY HARM', 'ATTEMPT AGG CRIM SEX ASSAULT/FELONY', 'ATTEMPT AGG CRIM SEX ASSAULT/VICTIM >60', 'ATTEMPT AGG CRIM SEX ASSAULT/WEAPON', 'ATTEMPT AGG CRIM SX AB/>5 YR OLDER VIC', 'ATTEMPT CRIM SEX ASSAULT/FORCE', 'ATTEMPT CRIM SEX ASSAULT/VICTIM 13-17', 'ATTEMPT PRED CRIM SEX ASLT/VICTIM <13', 'ATTEMPT PRED CRIMINAL SEXUAL ASSAULT', 'SOL AGG CRIM SEX ASSAULT/VIC 60+']\n",
    "drugs_list = ['15<200 OBJECT/PARTS LSD/ANALOG', 'AGG DEL METH PROTECTED <5 GRAMS', 'AGG DEL METH/PROTECTED/100+ GR', 'AGG DEL METH/PROTECTED/5<15 GRAMS', 'AGG METH MANU/ORGANIZE/100<400 GR', 'AGG METH MANU/PROTECTED<15', 'AGG METH MANU/WORSHIP/<15 GR', 'AGG METH MANUF/APT/<15 GRAMS', 'AGG METH MANUF/APT/100<400 GR', 'AGG METH MANUF/CHILD 15<100 GRAMS', 'AGG METH MANUF/CHILD/<15 GRAMS', 'AGG METH MANUF/CHLD/100<400 GRAMS', 'AGG METH MANUF/CHLD/400+ GR', 'AGG METH MANUF/ORGANIZE/<15 GR', 'CALCULATED CRIM DRUG CONSP', 'CALCULATED CRIM DRUG CONSPIR', 'CANNABIS TRAFFICKING', 'CONT SUBS ACT-MANU/DEL', 'CONTROLLED SUB TRAFFICKING', 'CRIM DRUG CONSPIRACY', 'CRIMINAL DRUG CONSPIRACY', 'CRIMINAL DRUG CONSPIRACY >100 GR', 'DEL CONT SUB<18/PARK/SCHOOL/PUB HOUS', 'DEL CONT/COUNT SUB TO <18', 'DEL METH <5 GRAMS', 'DEL/NON NARC SCHED I/II/SC/HS/PARK', 'DISPOSE METH MANUF WASTE', 'MAN/DEL OTHER AMOUNT AMPHETAMINE', 'MANU 5>15 GRAMS ECSTASY', 'MANU/DEL 01-15 GRAMS COCAINE', 'MANU/DEL 1-14 GRAMS HEROIN', 'MANU/DEL 1<15 GRAM FENTANYL', 'MANU/DEL 10-15 GRAMS HEROIN', 'MANU/DEL 10<15 OBJECTS/PARTS LSD', 'MANU/DEL 10>15 PILLS ECSTASY', 'MANU/DEL 100<400 GR FENTANYL', 'MANU/DEL 100<400 GRAMS COCAINE', 'MANU/DEL 100<400 GRAMS HEROIN', 'MANU/DEL 100<400 GRAMS LSD', 'MANU/DEL 100>400 GR ECSTASY', 'MANU/DEL 15/+ GRAMS HEROIN', 'MANU/DEL 15/+GRAMS COCAINE', 'MANU/DEL 15<100 GRAMS COCAINE', 'MANU/DEL 15<100 GRAMS HEROIN', 'MANU/DEL 15<100 GRAMS LSD', 'MANU/DEL 15>100 GR ECSTASY', 'MANU/DEL 15>200 PILLS ECSTASY', 'MANU/DEL 1500+ PILLS ECSTASY', 'MANU/DEL 200+ SCHED I&II', 'MANU/DEL 200+GRAMS METH/AMPH', 'MANU/DEL 200>600 PILLS ECSTASY', 'MANU/DEL 2ND OR SUBQ OFFENSE IN ACT', 'MANU/DEL 3-15 GRAMS LSD', 'MANU/DEL 400<900 GRAMS COCAINE', 'MANU/DEL 400<900 GRAMS METH', 'MANU/DEL 50-200 GRAMS SCHED I&II', 'MANU/DEL 600<1500 OBJECTS/PARTS LSD', 'MANU/DEL 900+ GRAMS COCAINE', 'MANU/DEL 900+ GRAMS ECSTASY', 'MANU/DEL 900+ GRAMS HEROIN', 'MANU/DEL AMT NARC SCHED I/II/SCHOOL/HS/PARK', 'MANU/DEL BARB ACID/SC/PUB HOUS/PARK', 'MANU/DEL CANNABIS/>5,000 GRAMS', 'MANU/DEL CANNABIS/>500 GRAMS', 'MANU/DEL CANNABIS/10-30 GRAMS', 'MANU/DEL CANNABIS/2000<5000 GRAMS', 'MANU/DEL CANNABIS/30-500 GRAMS', 'MANU/DEL COCAINE/SCHOOL/PUB HOUS/PARK', 'MANU/DEL CONT SUBS (PRIOR 1/1/90)', 'MANU/DEL CONT SUBS/ENHANCED', 'MANU/DEL HEROIN/SCHOOL/PUB HOUS/PARK', 'MANU/DEL OTHER AMOUNT SCHEDULE III', 'MANU/DEL OTHER AMOUNT SCHEDULE IV', 'MANU/DEL OTHER AMT METH', 'MANU/DEL OTHER AMT NARC SCHED I&II', 'MANU/DEL OTHER AMT NARCOTIC SCHED I&II', 'MANU/DEL OTHER NON-NARCOTIC SCHED I&II', 'MANU/DEL SCHED I/II/SCH/HS/PARK', 'MANU/DISTRIB LOOK-ALIKE SUB', 'METH CONSP CONT SUBS >100 GR', 'METH DELIVERY/100<400 GR', 'METH DELIVERY/15<100 GRAMS', 'METH DELIVERY/400<900 GR', 'METH DELIVERY/5<15 GRAMS', 'METH DELIVERY/900+ GR', 'METH DELIVERY<5 GRAMS', 'METH MANUFACTURE/100<400 GR', 'METH MANUFACTURE<15 GRAMS', 'METH MANUFACTURING/100<400 GRAMS', 'METH MANUFACTURING/15<100 GR', 'METH MANUFACTURING/15<100 GRAMS', 'METH MANUFACTURING/400<900 GR', 'METH MANUFACTURING/400<900 GRAMS', 'METH MANUFACTURING/900+ GR', 'METH MANUFACTURING<15 GRAMS', 'METH PRECURSOR TRAFFICKING', 'METH PRECURSOR/15<30 GRAMS', 'METH PRECURSOR/500+ GRAMS', 'METH PRECURSOR<15 GRAMS', 'METHAMPHETAMINE CONSPIRACY', 'METHAMPHETAMINE TRAFFICKING', 'MFG/DEL 100<400 GR HERO/ANLG', 'MFG/DEL 15<100 GR FENTANYL', 'MFG/DEL 15<100 GR HEROIN/ANLG', 'MFG/DEL 400<900 GR HERO/ANLG', 'NARCOTICS RACKETEERING', 'OBTAIN SUBSTANCE BY FRAUD/1ST', 'POSS AMT CON SUB EXCEPT(A)/(D)', 'POSS CANNABIS/>5,000 GRAMS', 'POSS CANNABIS/10-30 GRAM/SUBQ', 'POSS CANNABIS/2,000<5,000 GRAMS', 'POSS CANNABIS/500<2,000 GRAMS', 'POSS CONT SUBS', 'POSS HYPO/SYRINGE/NEEDLES/2ND+', 'POSS METH MANUF MATERIAL', 'POSS OF METH < 5 GRAMS', 'POSS OF METH/ 15<100 GRAMS', 'POSSESS 100<400 GRAMS COCAINE', 'POSSESS 100<400 GRAMS HEROIN', 'POSSESS 15+ GRAMS COCAINE', 'POSSESS 15+ GRAMS HEROIN', 'POSSESS 15+ GRAMS MORPHINE', 'POSSESS 15<100 GRAMS COCAINE', 'POSSESS 15<100 GRAMS HEROIN', 'POSSESS 15<100 GRAMS METH', 'POSSESS 15<200 OBJECT/PART LSD', 'POSSESS 15>200 PILLS ECSTASY', 'POSSESS 1500+ PILLS ECSTASY/ANALOG', 'POSSESS 200+ GRAMS OTHER SCHED I&II', 'POSSESS 400<900 GRAMS COCAINE', 'POSSESS 400<900 GRAMS HEROIN', 'POSSESS 900 + GRAMS COCAINE', 'POSSESS 900 + GRAMS HEROIN', 'POSSESSION OF METH/100<400 GRAMS', 'POSSESSION OF METH/15<100 GRAMS', 'POSSESSION OF METH/400<900 GR', 'POSSESSION OF METH/5<15 GRA', 'POSSESSION OF METH/900+ GR', 'POSSESSION OF METH< 5 GRAMS', 'PRODUCE >200 CANNABIS PLANTS', 'TAMPER W/ANHYD AMM EQUIPMENT', 'USE OF DANGEROUS PLACE/CON SUB', 'USE VEH/STRUCTURE/PROP/METH', 'POSS BLANK COUNTERFEIT SCRIPT', 'POSS CANNABIS/30-500 GRAM/1ST', 'MANU/DEL CONT SUBS', 'MANU/DEL 10<30 GRAMS KETAMINE', 'POSS 400>900 GR ECSTASY/ANALOG', 'PRODUCE 5-20 CANNABIS PLANTS', 'ATT MANU 200>600 PILLS ECSTASY', 'ATT METH PRECURSOR/30<150 GR', 'ATTEMPT MANU/DEL 15/+GRAMS COCAINE', 'ATTEMPT MANU/DEL CANNABIS/>500 GRAMS', 'ATTEMPT MANU/DEL OTHER AMT NARC', 'ATTEMPT POSS METH MANUF MATERIAL', 'ATTEMPT POSSESS 15<100 GRAMS COCAINE', 'CONSP MANU/DEL 01-15 GRAMS COCAINE', 'CONSP MANU/DEL OTHER AMOUNT METH', 'CONSP MFG/DEL CANNABIS/>5,000 GRAMS', 'SOLICIT MANU/DEL 01-15 GRAMS COCAINE', 'ATTEMPT MANU/DEL 01-15 GRAMS COCAINE', 'CONSPIRACY METH MANUFACTURE<15 GRAMS', 'ATTEMPT POSS AMT CON SUB', 'CONSP MANU/DEL 15/+GRAMS COCAINE']\n",
    "property_list = ['AGG ARSON', 'AGG ARSON/INJURE FIRE/POLICE', 'AGG ARSON/KNOW PEOPLE PRESENT', 'AGG HOME FRAUD VIC >60 >%500', 'AGG ID THEFT/<$300/DISABLED', 'AGG ID THEFT/$10,000-$100,000/60 YRS+', 'AGG ID THEFT/$300-10,000/60 YRS +', 'AGG ID THEFT/AGE 60+/2+', 'AGG ID THFT/10K-100K/GANG ACTS', 'AGG ID THFT/ID STOLEN/VIC 60+', 'AGG INSURANCE FRAUD', 'AGGRAVATED ARSON', 'AGGRAVATED ARSON/BODILY HARM', 'AID/ABET/POSS/SELL STOLEN VEH', 'ARSON', 'ARSON/REAL/PERSONAL PROP>$1', 'BAD CHK/OBTAIN CON PROP/>150', 'BURGLARY', 'BURGLARY W/O CAUSING DAMAGE', 'BURGLARY/SCHOOL/PLACE WORSHIP', 'CAUSING A CATASTROPHE', 'CONTINUING FINANCE CRIME ENTRPRS', 'COUNTERFEIT CREDIT/DEBIT CARD', 'CRIM DMG TO PROP $300-10000', 'CRIMINAL DAMAGE $10000-$100000', 'DEFRAUD FINANC INST/$500-$10,000', 'DEFRAUD FINANCIAL INST/>$100K', 'FIN EXPLOIT ELDERLY/70+/$15K+', 'FINANCE EXPLOIT ELD/DISABL/$300-$5,000', 'FINANCIAL INSTITUTION ROBBERY', 'FORGERY /MAKE OR ALTER DOCUMENT', 'FORGERY/ISSUE DOCUMENT/1 UPC', 'FORGERY/ISSUE/DELIVER DOCUMENT', 'FORGERY/POSSESS W INTENT', 'ID THEFT/OBTAIN/3+ INDIV', 'ID THFT/OBTAIN INFO/COMMIT FEL', 'IDENTITY THEFT >$100,000', 'IDENTITY THEFT/<$300', 'IDENTITY THEFT/$10K - $100K', 'IDENTITY THEFT/$2000-$10,000', 'IDENTITY THEFT/$300-$2000', 'IDENTITY THEFT/KNOWS ID STOLEN', 'KNOW ID STOLEN/MILITARY VIC/2+', 'MOB ACTION', 'ORGANIZE/AGG VEH THEFT CONSPIR', 'ORGANIZER/FIN CRIME ENTRPRS', 'POSS STOLEN VEHICLE > $25,000', 'POSSESS BURGLARY TOOLS', 'RECEIVE/POSS/SELL STOLEN VEH', 'RECEIVE/POSS/SELL STOLEN VEHICLE', 'RESIDENTIAL ARSON', 'RESIDENTIAL BURGLARY', 'RET THEFT/DISP MERCH/<$300/PREV CONV', 'RET THEFT/DISP MERCH/>$300', 'RETAIL THEFT/DISP MERCH/<$150/2+', 'RETAIL THEFT/EMERGENCY EXIT/>$300', 'RETAIL THEFT/MOTOR FUEL/>$150', 'THEFT', 'THEFT <$300', 'THEFT <$300 PRIOR', 'THEFT <$300 WITH PRIOR', 'THEFT <$300/PRIOR CONVICT', 'THEFT <$300/SCHOOL/WORSHIP', 'THEFT >$10,000 <$100,000', 'THEFT >$10,000-$100,000', 'THEFT >$100,000-$500,000', 'THEFT >$100,000/SCHOOL/WORSHIP', 'THEFT >$300-$10,000', 'THEFT $300-$10,000/SCHOOL/WORSHIP', 'THEFT $300<$10,000', 'THEFT CONTROL INTENT <$500 PRIOR', 'THEFT CONTROL INTENT $500<$10,000', 'THEFT CONTROL INTENT 10K<100K', 'THEFT CONTROL OF PROPERTY <$500', 'THEFT DECEP INTENT $10,000<$100,000', 'THEFT DECEPTION INTENT 300<10,000', 'THEFT DECEPTION INTENT 500<10000', 'THEFT STOLEN <$300 PRIOR', 'THEFT STOLEN INTENT PERS <$300', 'THEFT STOLEN/>$300 <$10,000', 'THEFT/COIN OP MACHINE/2ND+', 'THEFT/CONTROL INTENT>$500K-1M', 'THEFT/CONTROL/INTENT >$1M', 'THEFT/DECEPTION > $100,000', 'THEFT/DECEPTION >$500,000', 'THEFT/DECEPTION/<$500 PRIOR', 'THEFT/DECEPTION/>$500 <10K', 'THEFT/EMER EXIT/<$300/PRECONV', 'THEFT/STOLEN/ <$500 PRIOR CONV', 'THEFT/STOLEN/>$10,000 <$100,000', 'THEFT/UNAUTHD CONTROL>$300<10000', 'THFT/SWITCH PRICE/<300/PRECONV', 'UNLAWFUL POSS CREDIT/DEBIT CARD', 'UNLAWFUL POSS/DRIVER/VEH/STOLEN', 'UNLAWFUL POSS>3VEH/PARTS/STOLEN', 'USE FORGED CREDIT/DEBIT CARD/>$300', 'VEHICLE THEFT CONSPIRACY', 'FORGE REGISTRATION, ETC', 'ID THEFT/USE ID/COMMIT FELONY', 'IDENTITY THEFT $2,000<$10,000', 'THEFT/FALSE REP/<$300/PRECONV', 'USE OF ACCT NUMBER/CODE/>$150', 'AID/ABET CONCEAL/MISREP VEHICLE', 'RETAIL THEFT/EMERGENCY EXIT/<$300', 'HOME REPAIR FRAUD/CONTRACT >$1,000', 'THFT/LES RTN PROP/<300/PRECONV', 'USE OF ACCT NUMBER/CODE/<$150', 'CRIM DAMAGE PROP>$100,000', 'RET THEFT/MOTOR FUEL/<$150/PREV CONV', 'UNLAWFUL USE ID CARD/THEFT', 'ATT RECEIVE/POSS/SELL STOLEN VEH', 'ATTEMPT AGG ARSON', 'ATTEMPT ARSON', 'ATTEMPT ARSON >$150', 'ATTEMPT BURGLARY', 'ATTEMPT BURGLARY/SCHOOL/PLACE WORSHIP', 'ATTEMPT POSS STOLEN TITLE/CERTIF/PLATE', 'ATTEMPT POSS STOLEN VEHICLE > $25,000', 'ATTEMPT RESIDENTIAL ARSON', 'ATTEMPT RESIDENTIAL BURGLARY', 'ATTEMPT THEFT >$10,000-$100,000', 'CONSP RESIDENTIAL BURGLARY', 'ATTEMPT AID/ABET CONCEAL/MISREP VEHICLE']\n",
    "weapon_list = ['AGG POSS/11-20 STOLEN FIREARMS', 'AGG POSS/2-5 STOLEN FIREARMS', 'AGG UNLAWFUL USE OF WEAPON/VEH', 'AGG UNLAWFUL USE WEAPON/VEH/2ND', 'AGG UUW/ON PERSON', 'AGG UUW/PERS/FIR LOADED/FOID', 'AGG UUW/PERSON/PREV CONVICTION', 'AGG UUW/VEH/PREV CONVICTION', 'ARMED HABITUAL CRIMINAL', 'FIREARM/AMMO ACT-POSS WO/ID CARD', 'FIREARM/FOID INVALID/NO ELIG', 'GUNRUNNING', 'POSS EXPLOSIVE/INCENDIARY DEVICE', 'POSS FIREARM W/ DEFACED SER NO', 'POSS FIREARM/FOID NO ISSUE/NO ELIG', 'POSS FIREARM/LAND/GANG MEMBER', 'POSS OF FIREARM BY GANG MEMBER', 'POSSESSION OF STOLEN FIREARM', 'UNLAWFUL POSS HANDGUN/DEL/<21', 'UUW-FELON POSS WEAPON/BODY ARMOR', 'UUW-FELON POSS/USE FIREARM PRIOR', 'UUW-FELON POSS/USE FIREARM/PAROLE', 'UUW-FELON POSS/USE WEAPON/FIREARM', 'UUW/CARRY/POSS FIREARM/SCHOOL', 'UUW/MACHINE GUN/AUTO WEAPON/VEH', 'UUW/RIFLE <16 IN/SHOTGUN <18', 'UUW/CARRY/POSS FIREARM/2ND+', 'AGG UUW/PERSON/LOADED FIREARM/2+', 'UUW/CARRY/POSS FIREARM/1ST', 'ATT AGG POSS/31+ STOLEN FIREARMS', 'ATT FELON POSS/USE WEAP/BODY ARMOR', 'ATTEMPT AGG UUW W/ BODY ARMOR/PRIVATE', 'ATTEMPT UUW/FELON POSS/USE FIREARM/PAROLE', 'ATT POSS FIREARM/VEHICLE/GANG MEMBER', 'ATTEMPT POSSESSION OF STOLEN FIREARM']\n",
    "vehicle_list = ['AGG DUI', 'AGG DUI - SCHOOL BUS DRIVER', 'AGG DUI / 5TH DUI', 'AGG DUI / NO VALID INSURANCE', 'AGG DUI LIC SUSP OR REVOKED', 'AGG DUI/ 4TH /BAC 0.16+', 'AGG DUI/3', 'AGG DUI/3/BAC .16+', 'AGG DUI/3/PASS <16', 'AGG DUI/3RD/LIC SUSP OR REV', 'AGG DUI/3RD+', 'AGG DUI/4', 'AGG DUI/4TH/LIC SUSP OR REVOKED', 'AGG DUI/5/BAC 0.16>', 'AGG DUI/6+', 'AGG DUI/6+/BAC 0.16+', 'AGG DUI/DO DRIVER LICENSE', 'AGG DUI/LIC SUSP OR REVOKED', 'AGG DUI/NO VALID DL', 'AGG DUI/NO VALID INS', 'AGGRAVATED DUI/3RD+ DUI', 'DUI/3RD/BAC 0.16+', 'DUI/4TH+/LIC SUSP OR REVOKE', 'DUI/6TH', 'AGG DUI NO VALID DRIVERS LICENSE', 'AGG DUI/3RD+ DUI', 'DRIV LIC REVOKED/RECK HOMIC/2', 'DRIVE REVOKED/RECK HOMIC', 'DRIVE REVOKED/RECKL HOMIC/3', 'DRIVE W REVOKED/RECK HOMIC/4+', 'DRIVING RVK/SUSP DUI/SSS 4-9', 'DRIVING W/ REVOKE/SUSPEND LICENSE 2ND+', 'DRIVING W/ SUSPEND/REVOKE LICENSE 10-14', 'DRIVING W/ SUSPEND/REVOKE LICENSE 15+', 'DRIVING W/ SUSPEND/REVOKE LICENSE 2ND', 'DRIVING W/ SUSPEND/REVOKE LICENSE 3RD', 'DRIVING W/ SUSPEND/REVOKE LICENSE 4-9', 'REVOKED/SUSPENDED 2ND DUI']\n",
    "registry_list = ['VIOLENT OFF/YOUTH FAIL REGIS NEW RESID/SCHOOL', 'VIOLENT OFF/YOUTH FAIL REGIS/FAIL TO REPORT', 'VIOLENT OFF/YOUTH FAIL REGISTER IN ILLINOIS', 'VIOLENT OFF/YOUTH FAIL REGIS IN ILLINOIS 2+', 'CHIL SEX OFFEN/RESIDE SAY CARE', 'CHILD SEX OFFENDER/PUBLIC PARK', 'CHILD SEX OFFENDER/RESIDE 500FT', 'FAIL REGIS/NEW REDIS/SCH/2ND+', 'FAIL TO REPORT ANNUALLY/2+', 'SEX OFF FAIL REGISTER NEW RESIDENCE', 'SEX OFF FAIL REPORT CHANGE ADDR/EMPL', 'SEX OFF FAIL REPORT CHANGE ADDRESS/EMPLOY', 'SEX OFF FAIL TO REPORT ANNUALLY', 'SEX OFF FAIL TO REPORT WEEKLY/2ND', 'SEX OFFENDER AT PUBLIC PARK', 'SEX OFFENDER GIVE FALSE INFO/2ND', 'SEX OFFENDER REGIS VIOLATION', 'SEX OFFENDER REGIS/FALSE INFO', 'SEX VIOL/DANGEROUS FAIL TO RPT', 'PHOTO/DIGITAL IMAGE OF CHILD BY SEX OFFENDER', 'ATT SEX OFF FAIL REPORT/NO ADDR/2', 'ATT VIOL SEX OFFENDER REGIS ACT']\n",
    "other_list = ['AGG FLEEING POLICE/21 MPH OVER', 'AGG FLEEING POLICE/2ND', 'AGG FLEEING/2+ CON DVC/2ND', 'AGG FLEEING/2+ DISOBEY TRAFFIC DEVICES', 'AGG FLEEING/21 MPH OVER/2ND', 'AGG FLEEING/DAMAGE >$300 PROPERTY', 'BRIBERY/OFFER BRIBE', 'BRING CANNABIS IN A PENAL INST', 'BRING CON SUBSTANCE PENAL INST', 'BRING TOOL IN A PENAL INSTITUTION', 'BRING/POSS CONTRAB IN PENAL INST', 'CAUSE/BRG CON SUB PENAL INST', 'CONCEAL HOMICIDAL DEATH', 'CONCEAL/AID FUGITIVE', 'CONSP AGAINST CIVIL RIGHTS', 'DISARM PC OFF/CORR INST EMP', 'DISARMING A PEACE OFFICER', 'ESCAPE FROM DEPT OF CORRECTION', 'ESCAPE OF FELON FROM PENAL INST', 'ESCAPE/VIOLATE ELEC MONITORING', 'FAIL REPORT ACCIDENT/DEATH/INJURY', 'FAILURE REPORT ACCIDENT/DEATH', 'FAILURE REPORT ACCIDENT/INJURY', 'FALSE ALARM/COMPLAINT TO 911', 'FALSE PERSONATION OF POLICE', 'FALSE REPORT OF OFFENSE', 'FELON ESCAPE/PEACE OFFICER', 'FELON FAIL TO RETURN FROM FURLOUGH', 'FELON PROBATIONER ESCAPE OFF', 'FLEEING/ATTEMPT ELUDE OFF 3+', 'INDIRECT CRIMINAL CONTEMPT', 'MANUF/POSS ADULTERANTS DRUG TEST', 'MONEY LAUNDERING >$500,000', 'OBSTRUCT JUST/DESTROY EVIDENCE', 'OBSTRUCTING JUSTICE', 'POSS CONTROL SUBST PENAL INST', 'POSS CONTROL SUBSTANCE IN PENAL INST', 'POSS FRAUDULENT ID CARD', 'POSS/FRAUD/DL/PERMIT', 'POSSESS CANNIBIS PENAL INST', 'POSSESS ELEC CONTRABAND PENAL INST', 'POSSESS WEAPON IN PENAL INST', 'RESIST/OBSTRUC OFFICER/INJURY', 'RICO ACQUIRE ENTERPRISE', 'RICO CONSPIRACY TO VIOLATE', 'TERRORISM/FALSE THREAT', 'THREATEN PUBLIC OFFICIAL', 'THREATEN PUBLIC OFFICIAL/2ND', 'UUW-FELON POSS WEAPON IN PRISON', 'VENDOR FRAUD/KICKBACKS VIOL', 'CRIM DAMAGE/GOVT PROP/EXPL/<$500', 'CRIM DMG/GOVT PROP/<$500', 'CRIM DMG/GOVT PROP/>$500-$10K', 'RESIST/OBSTRUCT OFFICER/INJURY', 'UUW-FELON POSS/USE WEAP/PRISON', 'CRIM DAMAGE/STATE PROP/FIRE>$100,000', 'POSS/DISP ID CARD TO ANOTHER', 'ATT FOIL/DEFEAT SCREEN TEST', 'ATTEMPT ESCAPE FELON FROM PENAL INST', 'AGGRAVATED CRUELTY TO ANIMALS', 'ANIMAL TORTURE/2ND', 'CRUELTY TO ANIMALS/2ND', 'DOG FIGHTING/ATTEND SHOW']\n",
    "\n",
    "df.loc[df['Holding Offense'].isin(inj_death_list), 'Holding Offense Cat'] = 'Person - Death/Physical Injury'\n",
    "df.loc[df['Holding Offense'].isin(inj_death_unclear_list), 'Holding Offense Cat'] = 'Person - Death/Physical Injury Unclear'\n",
    "df.loc[df['Holding Offense'].isin(sex_list), 'Holding Offense Cat'] = 'Sex'\n",
    "df.loc[df['Holding Offense'].isin(drugs_list), 'Holding Offense Cat'] = 'Drugs'\n",
    "df.loc[df['Holding Offense'].isin(property_list), 'Holding Offense Cat'] = 'Property'\n",
    "df.loc[df['Holding Offense'].isin(weapon_list), 'Holding Offense Cat'] = 'Firearms/Weapon'\n",
    "df.loc[df['Holding Offense'].isin(vehicle_list), 'Holding Offense Cat'] = 'DUI/Vehicle Violation Without Death/Physical Injury'\n",
    "df.loc[df['Holding Offense'].isin(registry_list), 'Holding Offense Cat'] = 'Sex Offender/MVOAY Registry Violation'\n",
    "df.loc[df['Holding Offense'].isin(other_list), 'Holding Offense Cat'] = 'State/Other'\n",
    "\n",
    "\n",
    "# categorize unlisted offenses based on \"like\" criteria\n",
    "# ----- these won't be 100% accurately categorized, but it should be close -----\n",
    "offense_cat_dict = {'SEX OFF': 'Sex Offender/MVOAY Registry Violation',\n",
    "                    'SEX': 'Sex',\n",
    "                    'INDEC': 'Sex',\n",
    "                    'MURDER': 'Person - Death/Physical Injury',\n",
    "                    'HOMIC': 'Person - Death/Physical Injury',\n",
    "                    'HARM': 'Person - Death/Physical Injury',\n",
    "                    'PORN': 'Sex',\n",
    "                    'DUI': 'DUI/Vehicle Violation Without Death/Physical Injury',\n",
    "                    'ASSAULT': 'Person - Death/Physical Injury Unclear',\n",
    "                    'BATTERY': 'Person - Death/Physical Injury Unclear',\n",
    "                    'DISCH': 'Firearms/Weapon',\n",
    "                    'FIREA': 'Firearms/Weapon',\n",
    "                    'THEFT': 'Property',\n",
    "                    'STOLE': 'Property',\n",
    "                    'METH': 'Drugs',\n",
    "                    'KINAP': 'Person - Death/Physical Injury Unclear',\n",
    "                    'ROBB': 'Property',\n",
    "                    'WEAP': 'Firearms/Weapon',\n",
    "                    'GUN': 'Firearms/Weapon',\n",
    "                    'POSS': 'Drugs',   # this one especially contains non-drug offenses\n",
    "                    'ARSON': 'Property',\n",
    "                    'ABD': 'Person - Death/Physical Injury Unclear',\n",
    "                    'DEL': 'Drugs',\n",
    "                    'MANS': 'Person - Death/Physical Injury',\n",
    "                    'MAN': 'Drugs',\n",
    "                    'ENDANGER': 'Person - Death/Physical Injury Unclear',\n",
    "                    'CANNA': 'Drugs',\n",
    "                    'PROP': 'Property',\n",
    "                    'DRUG': 'Drugs'\n",
    "                    }\n",
    "\n",
    "# for each key word, label all holding offenses that contain that key word\n",
    "for key, val in offense_cat_dict.items():    \n",
    "    df.loc[(df['Holding Offense'].str.contains(key)) & (pd.isnull(df['Holding Offense Cat'])), 'Holding Offense Cat'] = val\n",
    "\n",
    "    \n",
    "# label those that are still uncategorized as other\n",
    "df.loc[pd.isnull(df['Holding Offense Cat']), 'Holding Offense Cat'] = 'State/Other'\n",
    "\n",
    "\n",
    "if len(df) != len1:\n",
    "    print('ERROR: categorizing offenses changed the df length')\n",
    "    1/0\n",
    "    \n",
    "print(df['Holding Offense Cat'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae636d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prep for sankey diagram\n",
    "holding_sankey_df = df[['Holding Offense Cat', 'Offense Type', 'IDOC #', 'Current Admission Date']].drop_duplicates()\n",
    "holding_sankey_df = holding_sankey_df[['Holding Offense Cat', 'Offense Type', 'IDOC #']].groupby(['Holding Offense Cat', 'Offense Type']).count().reset_index().rename(columns={'IDOC #': 'Count'})\n",
    "print(holding_sankey_df)\n",
    "\n",
    "# get the node df - a df dictionary of admission types and indices\n",
    "node_df = pd.concat([holding_sankey_df[['Holding Offense Cat']].rename(columns={'Holding Offense Cat': 'AT'}),\n",
    "                       holding_sankey_df[['Offense Type']].rename(columns={'Offense Type': 'AT'})])\n",
    "node_df = node_df.drop_duplicates().reset_index(drop=True).reset_index().rename(columns={'index': 'ID'})\n",
    "node_df['Color'] = get_hex(len(node_df))\n",
    "\n",
    "# get the links df\n",
    "links_df = pd.merge(holding_sankey_df, \n",
    "                    node_df[['AT', 'ID', 'Color']].rename(columns={'ID': 'Source', 'Color': 'Link Color'}), \n",
    "                    left_on='Holding Offense Cat', right_on='AT', how='inner')\n",
    "links_df = pd.merge(links_df, \n",
    "                    node_df[['AT', 'ID']].rename(columns={'ID': 'Target'}), \n",
    "                    left_on='Offense Type', right_on='AT', how='inner')\n",
    "links_df['Link Color'] = links_df['Link Color'].apply(lambda x: 'rgba' + str(get_rgb_from_hex(x) + (0.5,)))\n",
    "links_df = links_df[['Source', 'Target', 'Link Color', 'Count']]\n",
    "\n",
    "# create the chart\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 20,\n",
    "        thickness = 30,\n",
    "        line = dict(\n",
    "            color = 'black',\n",
    "            width = 0\n",
    "        ),\n",
    "        label = node_df['AT'],\n",
    "        color = node_df['Color']\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = links_df['Source'],\n",
    "        target = links_df['Target'],\n",
    "        value = links_df['Count'],\n",
    "        color = links_df['Link Color']\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Difference between Phil's Categories and IDOC Offense Type\", \n",
    "                  height=1000, \n",
    "                  font_size=12)\n",
    "fig.show()\n",
    "                                                                                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30a86e",
   "metadata": {},
   "source": [
    "Phil's categories match pretty well with IDOC's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18295cd",
   "metadata": {},
   "source": [
    "**Recidivism calculation**\n",
    "\n",
    "IDOC changed the way that it records recidivism over the years. Recalculate recidivism as sentence number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f036ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert admission date to a datetime\n",
    "df['Current Admission Date'] = pd.to_datetime(df['Current Admission Date'])\n",
    "\n",
    "len1 = len(df)\n",
    "\n",
    "# rank the admission date per person - this is the nth time they've been admitted\n",
    "df_admis_rank = df[['IDOC #', 'Current Admission Date']].drop_duplicates()\n",
    "df_admis_rank['Sentence Number'] = df_admis_rank.groupby(['IDOC #'])['Current Admission Date'].rank(ascending=True)\n",
    "\n",
    "# join back to the full dataframe\n",
    "df = pd.merge(df, df_admis_rank, on=['IDOC #', 'Current Admission Date'], how='inner')\n",
    "\n",
    "if len(df) != len1:\n",
    "    print('ERROR: Adding the sentence number changed the df length')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c69ccb",
   "metadata": {},
   "source": [
    "**Eligibility for Sentencing Credits**\n",
    "\n",
    "For the purposes of our analysis, someone is *ineligible* for earned discretionary sentencing credits (EDSC) if:\n",
    "* Their holding offense is related to gunrunning or sex\n",
    "* They're sentenced to life\n",
    "* They're required to serve at least 85% of their sentence under truth-in-sentencing\n",
    "* They're considered a sexually dangerous person (SDP)\n",
    "\n",
    "Otherwise, they are eligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c146dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EDSC Eligible'] = None\n",
    "df.loc[(((df['TIS Pct'] >= 0.85) & (~pd.isnull(df['TIS Pct'])))\n",
    "      | (df['Holding Offense'].str.contains('GUNRUNNING'))\n",
    "      | (df['Sentence Years'].isin(['LIFE', 'SDP']))\n",
    "      | (df['Holding Offense Cat'] == 'Sex')),\n",
    "      'EDSC Eligible'] = 'N'\n",
    "\n",
    "df.loc[pd.isnull(df['EDSC Eligible']), 'EDSC Eligible'] = 'Y'\n",
    "\n",
    "print(df['EDSC Eligible'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a40fac",
   "metadata": {},
   "source": [
    "**Exporting the data**\n",
    "\n",
    "The full data set is 1.8 million records. Export with understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6653d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export more recent data\n",
    "# df_recent = df[df['Doc Date'] >= date(2014, 1, 1)]\n",
    "# print(len(df_recent))\n",
    "# df_recent.to_csv('pop_data_2014_thru_20220331.csv')\n",
    "\n",
    "# # export single institution all time\n",
    "# df_export = df[df['Parent Institution'] == 'Logan']\n",
    "# print(len(df_export))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ad79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_recent.to_csv('pop_data_20140101-20211231.csv')\n",
    "df.to_csv('pop_data_thru_20220930.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[(df['Doc Date'].apply(lambda x: x.date()) >= date(2022, 2, 1))] #& (df['Doc Date'] <= date(2018, 9, 30))]\n",
    "print(len(tmp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.to_csv('tmp_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
